{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikaf-CwjNRPG",
        "colab_type": "text"
      },
      "source": [
        "# 0. Importations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgdRco0F_0QJ",
        "colab_type": "code",
        "outputId": "93f59905-f924-45f2-a37d-6bb88e6eb078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv3v7l0R_8T4",
        "colab_type": "code",
        "outputId": "493c0947-4779-4fd6-97a3-4898589178cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import os\n",
        "print(f\"Chemin vers le dossier actuel: \\n {os.getcwd()}\")\n",
        "path_main_folder = '/content/drive/My Drive/lyrics_generation/data/'\n",
        "\n",
        "os.chdir(path_main_folder)\n",
        "print(f\"Chemin vers le dossier actuel: \\n {os.getcwd()}\")"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chemin vers le dossier actuel: \n",
            " /content/drive/My Drive/lyrics_generation/data\n",
            "Chemin vers le dossier actuel: \n",
            " /content/drive/My Drive/lyrics_generation/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNYDItefAFi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from argparse import Namespace\n",
        "import random\n",
        "import string\n",
        "import io\n",
        "import sys, os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCoe7RbCASeG",
        "colab_type": "text"
      },
      "source": [
        "Define device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC9bPRqVVDz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDGGtBJyNXck",
        "colab_type": "text"
      },
      "source": [
        "# 1. Text pre-processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBM0ev3AAdx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "    text = text.replace(\"'ll\",' will')\n",
        "    text = text.replace(\"'m\",' am')\n",
        "    text = text.replace(\"'re\",' are')\n",
        "    text = text.replace(\"'s\",' is')\n",
        "    text = text.replace(\"\\n\",\" \\n \")\n",
        "    text = text.lower()\n",
        "    text = re.sub(' {2,}', ' ', text)\n",
        "    #text = \"\".join(v for v in text if v not in string.punctuation).lower()\n",
        "    #text = text.replace(\"\\n\",\" \\SAUT \")\n",
        "    return text\n",
        "\n",
        "def extract_characters(text):\n",
        "    return sorted(list(set(text)))\n",
        "\n",
        "def get_chars_index_dicts(chars):\n",
        "    return dict((c, i) for i, c in enumerate(chars)), dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYnKH9giN_xS",
        "colab_type": "text"
      },
      "source": [
        "# 2. Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omsBXGzqQe69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encode characters in numbers\n",
        "def vectorize(sentence, char_to_index):\n",
        "  return np.array([char_to_index[char] for char in sentence])\n",
        "\n",
        "#Project a batch of int_sequences in dim chars space\n",
        "def one_hot_encode(sequences, chars):\n",
        "    X = np.zeros((len(sequences), len(sequences[0]), len(chars)), dtype=int)\n",
        "    for i, sentence in enumerate(sequences):\n",
        "        for t, integer in enumerate(sentence):\n",
        "            X[i, t, integer] = 1\n",
        "    return X\n",
        "\n",
        "# def one_hot_encode(sequences, sequence_length, chars, char_to_index, next_chars):\n",
        "#     X = np.zeros((len(sequences), sequence_length, len(chars)), dtype=int)\n",
        "#     y = np.zeros((len(sequences), len(chars)), dtype=int)\n",
        "#     for i, sentence in enumerate(sequences):\n",
        "#         for t, char in enumerate(sentence):\n",
        "#             X[i, t, char_to_index[char]] = 1\n",
        "#         y[i, char_to_index[next_chars[i]]] = 1\n",
        "\n",
        "#     return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7VoDFaDrkR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(train_loader):\n",
        "    # obtain one batch of training data\n",
        "    dataiter = iter(train_loader)\n",
        "    sample_x, sample_y = dataiter.next()\n",
        "    print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "    print('Sample input: \\n', sample_x)\n",
        "    print()\n",
        "    print('Sample output size: ', sample_y.size()) # batch_size\n",
        "    print('Sample output: \\n', sample_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhETnRtsRWz4",
        "colab_type": "text"
      },
      "source": [
        "# Data extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep-zkc-yReSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_file(train_file,artist_name=None):\n",
        "    \n",
        "    df = pd.read_csv(train_file)\n",
        "    \n",
        "    #Select artist\n",
        "    if artist_name :\n",
        "      df = df[df.artist==artist_name]\n",
        "    \n",
        "    #Get text\n",
        "    text = ' '.join([x for x in df.text])\n",
        "\n",
        "    #Transform data\n",
        "    text = preprocess(text)\n",
        "    \n",
        "    #Get characters\n",
        "    chars = extract_characters(text)\n",
        "\n",
        "    #Get dictionnaries\n",
        "    char_to_index, indices_char = get_chars_index_dicts(chars)\n",
        "\n",
        "    #Encode text\n",
        "    encoded_text = vectorize(text, char_to_index)\n",
        "\n",
        "    return chars, encoded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXfEMBsMTBa9",
        "colab_type": "text"
      },
      "source": [
        "# Create Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfK3g6sKTDw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBFkaRtpWr4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "76d37f47-df4c-4a9e-8567-a85b01000e4a"
      },
      "source": [
        "x, y = next(get_batches(encoded_text,10,40))\n",
        "print(x.shape)\n",
        "one_hot_encode(x,chars).shape"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 40)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 40, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K7tJnqQUsuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def create_sequences(text, sequence_length, step):\n",
        "#     sequences = []\n",
        "#     next_chars = []\n",
        "#     for i in range(0, len(text) - sequence_length, step):\n",
        "#         sequences.append(text[i: i + sequence_length])\n",
        "#         next_chars.append(text[i + sequence_length])\n",
        "#     return sequences, next_chars\n",
        "\n",
        "# def create_sequences(text, sequence_length, step):\n",
        "#     text = text[:sequence_length * (len(text)//sequence_length)+2]\n",
        "#     sequences = []\n",
        "#     next_sequences = []\n",
        "#     for i in range(0, len(text) - sequence_length-1, step):\n",
        "#         sequences.append(text[i: i + sequence_length])\n",
        "#         next_sequences.append(text[i+1: i + sequence_length+1])\n",
        "#     return sequences, next_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKwmVCuBOSUj",
        "colab_type": "text"
      },
      "source": [
        "# 3. Create Model class and train function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azqpndqROvF_",
        "colab_type": "text"
      },
      "source": [
        "## 3. A) LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lKvf1MlFiCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, chars, hidden_size=128, num_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.chars = chars\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.drop_prob = drop_prob\n",
        "        self.lr = lr\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        # Define layers\n",
        "        self.lstm = nn.LSTM(input_size=len(chars), hidden_size=hidden_size, num_layers=num_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(hidden_size, len(chars))\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "      # Hidden = h et c les Ã©tats internes\n",
        "      \n",
        "      output, hidden = self.lstm(x, hidden)\n",
        "  \n",
        "      # Use dropout\n",
        "      out = self.dropout(output)\n",
        "\n",
        "      out = out.contiguous().view(-1, self.hidden_size)\n",
        "      #x = x[:,-1,:]  # Keep only the output of the last iteration. Before shape (6,3,128), after shape (6,128)\n",
        "      \n",
        "      out = self.fc(out)\n",
        "      #out = self.softmax(out)\n",
        "      \n",
        "      return out, hidden\n",
        "    \n",
        "      \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes nb_layers x batch_size x hidden_size,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        \n",
        "        hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_size, dtype=int).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size,dtype=int).to(device))\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z07TRaUYOzbT",
        "colab_type": "text"
      },
      "source": [
        "## 3. B) Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu3YONIuAug8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data, batch_size=10, seq_length=40, nb_epochs=8000, lr=0.01, clip=5, val_frac=0.1, print_every=1):\n",
        "  \n",
        "  model = model.to(device)\n",
        "  model.train()\n",
        "  \n",
        "  #criterion = nn.MSELoss()\n",
        "  #criterion = nn.CategoricalCrossEntropy()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "  #optimizer = torch.optim.RMSprop(model.parameters(), lr = lr)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  \n",
        "  # Create training and validation data\n",
        "  #TODO: cross-validation\n",
        "  val_index = int(len(data)*(1-val_frac))\n",
        "  train_data, val_data = data[:val_index], data[val_index:]\n",
        "\n",
        "  counter = 0\n",
        "  n_chars = len(model.chars)\n",
        "\n",
        "  for epoch in range(nb_epochs):\n",
        "    # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "\n",
        "    #train_data = TensorDataset(torch.from_numpy(X).to(device), torch.from_numpy(y).to(device))\n",
        "    #train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "    for x, y in get_batches(data, batch_size, seq_length):\n",
        "      \n",
        "      counter+=1\n",
        "\n",
        "      # One-hot encode our data and make them Torch tensors (only x data!!)  \n",
        "      x = one_hot_encode(x, model.chars)\n",
        "\n",
        "\n",
        "      inputs, targets = torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
        "      \n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      \n",
        "      # Creating new variables for the hidden state, otherwise\n",
        "      # we'd backprop through the entire training history\n",
        "      h = tuple([each.data.float() for each in h])\n",
        " \n",
        "      model.zero_grad()\n",
        "\n",
        "      output, h = model(inputs, h)\n",
        "      \n",
        "      loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "      #loss = criterion(output, targets)\n",
        "      \n",
        "      loss.backward()\n",
        "      \n",
        "      # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      # loss stats\n",
        "      if counter % print_every == 0:\n",
        "          # Get validation loss\n",
        "          val_h = model.init_hidden(batch_size)\n",
        "          val_losses = []\n",
        "          model.eval()\n",
        "          for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "              # One-hot encode our data and make them Torch tensors\n",
        "              x = one_hot_encode(x, model.chars)\n",
        "              inputs, targets = torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
        "              \n",
        "              # Creating new variables for the hidden state, otherwise\n",
        "              # we'd backprop through the entire training history\n",
        "              val_h = tuple([each.data.float() for each in val_h])\n",
        "            \n",
        "              inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "              output, val_h = model(inputs, val_h)\n",
        "              val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "          \n",
        "              val_losses.append(val_loss.item())\n",
        "          \n",
        "          model.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "          print(\"Epoch: [{}/{}]\".format(epoch+1, nb_epochs),\n",
        "                \"Step: [{}]\".format(counter),\n",
        "                \"Loss: {:.4f}\".format(loss.item()),\n",
        "                \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "\n",
        "    #print(f\"Epoch {epoch+1}/{n_epochs}, loss = {loss.item()}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jXFz7ZjIaXz",
        "colab_type": "text"
      },
      "source": [
        "# 4. Instantiate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUahDdWnO50g",
        "colab_type": "text"
      },
      "source": [
        "## 4. A) Pipeline : from text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwCqQQBlPy13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flags = Namespace(\n",
        "    train_file='reducedsongdata.csv',\n",
        "    artist_name='Coldplay',\n",
        "    batch_size=8,\n",
        "    SEQUENCE_LENGTH=40,\n",
        "    embedding_size=1,\n",
        "    hidden_size=32,\n",
        "    gradients_norm=5,\n",
        "    initial_words=['i', 'am'],\n",
        "    predict_top_k=5,\n",
        "    checkpoint_path='checkpoint',\n",
        "    n_layers=1,\n",
        "    SEQUENCE_STEP = 1,\n",
        "    EPOCHS10 = 10,\n",
        "    DIVERSITY = 1.0\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01wmppOoPpxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars, encoded_text = get_data_from_file(flags.train_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VGCwr9fU3Ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTM(chars,hidden_size=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXs4iBvxU55i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model =model.float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnzKxXI_U0-x",
        "colab_type": "code",
        "outputId": "38887576-c889-46f8-b359-9c919c92c9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Train the model\n",
        "\n",
        "batch_size = 128\n",
        "seq_length = 40\n",
        "n_epochs = 1000\n",
        "\n",
        "train(model, encoded_text, batch_size=batch_size, seq_length=seq_length, nb_epochs=n_epochs, lr=0.01, print_every=20)\n",
        "#train(model, X, y, batch_size=128, n_epochs=EPOCHS10)"
      ],
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/1000] Step: [20] Loss: 1.2420 Val Loss: 1.1570\n",
            "Epoch: [1/1000] Step: [40] Loss: 1.2128 Val Loss: 1.1543\n",
            "Epoch: [2/1000] Step: [60] Loss: 1.2201 Val Loss: 1.1483\n",
            "Epoch: [2/1000] Step: [80] Loss: 1.2579 Val Loss: 1.1431\n",
            "Epoch: [2/1000] Step: [100] Loss: 1.2446 Val Loss: 1.1382\n",
            "Epoch: [3/1000] Step: [120] Loss: 1.2294 Val Loss: 1.1402\n",
            "Epoch: [3/1000] Step: [140] Loss: 1.2071 Val Loss: 1.1322\n",
            "Epoch: [4/1000] Step: [160] Loss: 1.1704 Val Loss: 1.1385\n",
            "Epoch: [4/1000] Step: [180] Loss: 1.2027 Val Loss: 1.1252\n",
            "Epoch: [4/1000] Step: [200] Loss: 1.1810 Val Loss: 1.1278\n",
            "Epoch: [5/1000] Step: [220] Loss: 1.2219 Val Loss: 1.1226\n",
            "Epoch: [5/1000] Step: [240] Loss: 1.2147 Val Loss: 1.1157\n",
            "Epoch: [6/1000] Step: [260] Loss: 1.1710 Val Loss: 1.1126\n",
            "Epoch: [6/1000] Step: [280] Loss: 1.1955 Val Loss: 1.1158\n",
            "Epoch: [6/1000] Step: [300] Loss: 1.2153 Val Loss: 1.1165\n",
            "Epoch: [7/1000] Step: [320] Loss: 1.2062 Val Loss: 1.1158\n",
            "Epoch: [7/1000] Step: [340] Loss: 1.1625 Val Loss: 1.1025\n",
            "Epoch: [8/1000] Step: [360] Loss: 1.2046 Val Loss: 1.0993\n",
            "Epoch: [8/1000] Step: [380] Loss: 1.1733 Val Loss: 1.1073\n",
            "Epoch: [8/1000] Step: [400] Loss: 1.2047 Val Loss: 1.1060\n",
            "Epoch: [9/1000] Step: [420] Loss: 1.2248 Val Loss: 1.1065\n",
            "Epoch: [9/1000] Step: [440] Loss: 1.1851 Val Loss: 1.0948\n",
            "Epoch: [10/1000] Step: [460] Loss: 1.1749 Val Loss: 1.0938\n",
            "Epoch: [10/1000] Step: [480] Loss: 1.2153 Val Loss: 1.0930\n",
            "Epoch: [10/1000] Step: [500] Loss: 1.1774 Val Loss: 1.0873\n",
            "Epoch: [11/1000] Step: [520] Loss: 1.1985 Val Loss: 1.0885\n",
            "Epoch: [11/1000] Step: [540] Loss: 1.1471 Val Loss: 1.0846\n",
            "Epoch: [11/1000] Step: [560] Loss: 1.1182 Val Loss: 1.0855\n",
            "Epoch: [12/1000] Step: [580] Loss: 1.1870 Val Loss: 1.0858\n",
            "Epoch: [12/1000] Step: [600] Loss: 1.1442 Val Loss: 1.0827\n",
            "Epoch: [13/1000] Step: [620] Loss: 1.1627 Val Loss: 1.0747\n",
            "Epoch: [13/1000] Step: [640] Loss: 1.1364 Val Loss: 1.0806\n",
            "Epoch: [13/1000] Step: [660] Loss: 1.1554 Val Loss: 1.0701\n",
            "Epoch: [14/1000] Step: [680] Loss: 1.1598 Val Loss: 1.0735\n",
            "Epoch: [14/1000] Step: [700] Loss: 1.1694 Val Loss: 1.0631\n",
            "Epoch: [15/1000] Step: [720] Loss: 1.1094 Val Loss: 1.0713\n",
            "Epoch: [15/1000] Step: [740] Loss: 1.1136 Val Loss: 1.0610\n",
            "Epoch: [15/1000] Step: [760] Loss: 1.1239 Val Loss: 1.0583\n",
            "Epoch: [16/1000] Step: [780] Loss: 1.1364 Val Loss: 1.0705\n",
            "Epoch: [16/1000] Step: [800] Loss: 1.1151 Val Loss: 1.0594\n",
            "Epoch: [17/1000] Step: [820] Loss: 1.1195 Val Loss: 1.0523\n",
            "Epoch: [17/1000] Step: [840] Loss: 1.1122 Val Loss: 1.0623\n",
            "Epoch: [17/1000] Step: [860] Loss: 1.1431 Val Loss: 1.0630\n",
            "Epoch: [18/1000] Step: [880] Loss: 1.1390 Val Loss: 1.0614\n",
            "Epoch: [18/1000] Step: [900] Loss: 1.0833 Val Loss: 1.0533\n",
            "Epoch: [19/1000] Step: [920] Loss: 1.1284 Val Loss: 1.0573\n",
            "Epoch: [19/1000] Step: [940] Loss: 1.1585 Val Loss: 1.0580\n",
            "Epoch: [19/1000] Step: [960] Loss: 1.1302 Val Loss: 1.0502\n",
            "Epoch: [20/1000] Step: [980] Loss: 1.1425 Val Loss: 1.0494\n",
            "Epoch: [20/1000] Step: [1000] Loss: 1.1139 Val Loss: 1.0422\n",
            "Epoch: [20/1000] Step: [1020] Loss: 1.2595 Val Loss: 1.0484\n",
            "Epoch: [21/1000] Step: [1040] Loss: 1.1229 Val Loss: 1.0507\n",
            "Epoch: [21/1000] Step: [1060] Loss: 1.0688 Val Loss: 1.0441\n",
            "Epoch: [22/1000] Step: [1080] Loss: 1.0889 Val Loss: 1.0372\n",
            "Epoch: [22/1000] Step: [1100] Loss: 1.1275 Val Loss: 1.0369\n",
            "Epoch: [22/1000] Step: [1120] Loss: 1.1289 Val Loss: 1.0371\n",
            "Epoch: [23/1000] Step: [1140] Loss: 1.1235 Val Loss: 1.0400\n",
            "Epoch: [23/1000] Step: [1160] Loss: 1.1055 Val Loss: 1.0322\n",
            "Epoch: [24/1000] Step: [1180] Loss: 1.0832 Val Loss: 1.0298\n",
            "Epoch: [24/1000] Step: [1200] Loss: 1.0927 Val Loss: 1.0359\n",
            "Epoch: [24/1000] Step: [1220] Loss: 1.0714 Val Loss: 1.0275\n",
            "Epoch: [25/1000] Step: [1240] Loss: 1.1056 Val Loss: 1.0280\n",
            "Epoch: [25/1000] Step: [1260] Loss: 1.0885 Val Loss: 1.0321\n",
            "Epoch: [26/1000] Step: [1280] Loss: 1.0709 Val Loss: 1.0256\n",
            "Epoch: [26/1000] Step: [1300] Loss: 1.0918 Val Loss: 1.0334\n",
            "Epoch: [26/1000] Step: [1320] Loss: 1.1152 Val Loss: 1.0213\n",
            "Epoch: [27/1000] Step: [1340] Loss: 1.1008 Val Loss: 1.0263\n",
            "Epoch: [27/1000] Step: [1360] Loss: 1.0783 Val Loss: 1.0199\n",
            "Epoch: [28/1000] Step: [1380] Loss: 1.0973 Val Loss: 1.0203\n",
            "Epoch: [28/1000] Step: [1400] Loss: 1.0905 Val Loss: 1.0257\n",
            "Epoch: [28/1000] Step: [1420] Loss: 1.1204 Val Loss: 1.0159\n",
            "Epoch: [29/1000] Step: [1440] Loss: 1.1320 Val Loss: 1.0231\n",
            "Epoch: [29/1000] Step: [1460] Loss: 1.0986 Val Loss: 1.0164\n",
            "Epoch: [30/1000] Step: [1480] Loss: 1.0909 Val Loss: 1.0180\n",
            "Epoch: [30/1000] Step: [1500] Loss: 1.1348 Val Loss: 1.0233\n",
            "Epoch: [30/1000] Step: [1520] Loss: 1.0790 Val Loss: 1.0135\n",
            "Epoch: [31/1000] Step: [1540] Loss: 1.1210 Val Loss: 1.0134\n",
            "Epoch: [31/1000] Step: [1560] Loss: 1.0754 Val Loss: 1.0111\n",
            "Epoch: [31/1000] Step: [1580] Loss: 1.0389 Val Loss: 1.0096\n",
            "Epoch: [32/1000] Step: [1600] Loss: 1.1378 Val Loss: 1.0160\n",
            "Epoch: [32/1000] Step: [1620] Loss: 1.0939 Val Loss: 1.0046\n",
            "Epoch: [33/1000] Step: [1640] Loss: 1.0753 Val Loss: 1.0076\n",
            "Epoch: [33/1000] Step: [1660] Loss: 1.0755 Val Loss: 1.0126\n",
            "Epoch: [33/1000] Step: [1680] Loss: 1.0818 Val Loss: 0.9987\n",
            "Epoch: [34/1000] Step: [1700] Loss: 1.0992 Val Loss: 1.0063\n",
            "Epoch: [34/1000] Step: [1720] Loss: 1.0793 Val Loss: 0.9979\n",
            "Epoch: [35/1000] Step: [1740] Loss: 1.0221 Val Loss: 0.9991\n",
            "Epoch: [35/1000] Step: [1760] Loss: 1.0331 Val Loss: 1.0006\n",
            "Epoch: [35/1000] Step: [1780] Loss: 1.0513 Val Loss: 1.0038\n",
            "Epoch: [36/1000] Step: [1800] Loss: 1.0635 Val Loss: 1.0072\n",
            "Epoch: [36/1000] Step: [1820] Loss: 1.0552 Val Loss: 0.9947\n",
            "Epoch: [37/1000] Step: [1840] Loss: 1.0609 Val Loss: 0.9930\n",
            "Epoch: [37/1000] Step: [1860] Loss: 1.0643 Val Loss: 1.0015\n",
            "Epoch: [37/1000] Step: [1880] Loss: 1.0774 Val Loss: 0.9935\n",
            "Epoch: [38/1000] Step: [1900] Loss: 1.0813 Val Loss: 0.9990\n",
            "Epoch: [38/1000] Step: [1920] Loss: 1.0378 Val Loss: 0.9944\n",
            "Epoch: [39/1000] Step: [1940] Loss: 1.0618 Val Loss: 0.9924\n",
            "Epoch: [39/1000] Step: [1960] Loss: 1.0988 Val Loss: 1.0009\n",
            "Epoch: [39/1000] Step: [1980] Loss: 1.0557 Val Loss: 0.9962\n",
            "Epoch: [40/1000] Step: [2000] Loss: 1.0764 Val Loss: 0.9935\n",
            "Epoch: [40/1000] Step: [2020] Loss: 1.0739 Val Loss: 0.9951\n",
            "Epoch: [40/1000] Step: [2040] Loss: 1.2111 Val Loss: 0.9918\n",
            "Epoch: [41/1000] Step: [2060] Loss: 1.0673 Val Loss: 0.9924\n",
            "Epoch: [41/1000] Step: [2080] Loss: 1.0097 Val Loss: 0.9899\n",
            "Epoch: [42/1000] Step: [2100] Loss: 1.0450 Val Loss: 0.9886\n",
            "Epoch: [42/1000] Step: [2120] Loss: 1.0683 Val Loss: 0.9888\n",
            "Epoch: [42/1000] Step: [2140] Loss: 1.0707 Val Loss: 0.9862\n",
            "Epoch: [43/1000] Step: [2160] Loss: 1.0655 Val Loss: 0.9884\n",
            "Epoch: [43/1000] Step: [2180] Loss: 1.0620 Val Loss: 0.9911\n",
            "Epoch: [44/1000] Step: [2200] Loss: 1.0391 Val Loss: 0.9825\n",
            "Epoch: [44/1000] Step: [2220] Loss: 1.0541 Val Loss: 0.9879\n",
            "Epoch: [44/1000] Step: [2240] Loss: 1.0179 Val Loss: 0.9841\n",
            "Epoch: [45/1000] Step: [2260] Loss: 1.0816 Val Loss: 0.9849\n",
            "Epoch: [45/1000] Step: [2280] Loss: 1.0602 Val Loss: 0.9747\n",
            "Epoch: [46/1000] Step: [2300] Loss: 1.0364 Val Loss: 0.9803\n",
            "Epoch: [46/1000] Step: [2320] Loss: 1.0414 Val Loss: 0.9811\n",
            "Epoch: [46/1000] Step: [2340] Loss: 1.0600 Val Loss: 0.9851\n",
            "Epoch: [47/1000] Step: [2360] Loss: 1.0626 Val Loss: 0.9802\n",
            "Epoch: [47/1000] Step: [2380] Loss: 1.0252 Val Loss: 0.9752\n",
            "Epoch: [48/1000] Step: [2400] Loss: 1.0515 Val Loss: 0.9734\n",
            "Epoch: [48/1000] Step: [2420] Loss: 1.0642 Val Loss: 0.9814\n",
            "Epoch: [48/1000] Step: [2440] Loss: 1.0840 Val Loss: 0.9736\n",
            "Epoch: [49/1000] Step: [2460] Loss: 1.0995 Val Loss: 0.9807\n",
            "Epoch: [49/1000] Step: [2480] Loss: 1.0602 Val Loss: 0.9753\n",
            "Epoch: [50/1000] Step: [2500] Loss: 1.0541 Val Loss: 0.9710\n",
            "Epoch: [50/1000] Step: [2520] Loss: 1.0993 Val Loss: 0.9767\n",
            "Epoch: [50/1000] Step: [2540] Loss: 1.0274 Val Loss: 0.9757\n",
            "Epoch: [51/1000] Step: [2560] Loss: 1.0717 Val Loss: 0.9635\n",
            "Epoch: [51/1000] Step: [2580] Loss: 1.0249 Val Loss: 0.9666\n",
            "Epoch: [51/1000] Step: [2600] Loss: 0.9988 Val Loss: 0.9680\n",
            "Epoch: [52/1000] Step: [2620] Loss: 1.0773 Val Loss: 0.9726\n",
            "Epoch: [52/1000] Step: [2640] Loss: 1.0620 Val Loss: 0.9745\n",
            "Epoch: [53/1000] Step: [2660] Loss: 1.0592 Val Loss: 0.9674\n",
            "Epoch: [53/1000] Step: [2680] Loss: 1.0280 Val Loss: 0.9702\n",
            "Epoch: [53/1000] Step: [2700] Loss: 1.0555 Val Loss: 0.9694\n",
            "Epoch: [54/1000] Step: [2720] Loss: 1.0650 Val Loss: 0.9714\n",
            "Epoch: [54/1000] Step: [2740] Loss: 1.0369 Val Loss: 0.9663\n",
            "Epoch: [55/1000] Step: [2760] Loss: 1.0020 Val Loss: 0.9697\n",
            "Epoch: [55/1000] Step: [2780] Loss: 1.0087 Val Loss: 0.9735\n",
            "Epoch: [55/1000] Step: [2800] Loss: 1.0185 Val Loss: 0.9739\n",
            "Epoch: [56/1000] Step: [2820] Loss: 1.0377 Val Loss: 0.9695\n",
            "Epoch: [56/1000] Step: [2840] Loss: 1.0297 Val Loss: 0.9646\n",
            "Epoch: [57/1000] Step: [2860] Loss: 1.0193 Val Loss: 0.9608\n",
            "Epoch: [57/1000] Step: [2880] Loss: 1.0216 Val Loss: 0.9649\n",
            "Epoch: [57/1000] Step: [2900] Loss: 1.0439 Val Loss: 0.9688\n",
            "Epoch: [58/1000] Step: [2920] Loss: 1.0341 Val Loss: 0.9628\n",
            "Epoch: [58/1000] Step: [2940] Loss: 1.0061 Val Loss: 0.9615\n",
            "Epoch: [59/1000] Step: [2960] Loss: 1.0334 Val Loss: 0.9565\n",
            "Epoch: [59/1000] Step: [2980] Loss: 1.0634 Val Loss: 0.9621\n",
            "Epoch: [59/1000] Step: [3000] Loss: 1.0258 Val Loss: 0.9656\n",
            "Epoch: [60/1000] Step: [3020] Loss: 1.0533 Val Loss: 0.9631\n",
            "Epoch: [60/1000] Step: [3040] Loss: 1.0176 Val Loss: 0.9572\n",
            "Epoch: [60/1000] Step: [3060] Loss: 1.1565 Val Loss: 0.9537\n",
            "Epoch: [61/1000] Step: [3080] Loss: 1.0382 Val Loss: 0.9579\n",
            "Epoch: [61/1000] Step: [3100] Loss: 0.9775 Val Loss: 0.9601\n",
            "Epoch: [62/1000] Step: [3120] Loss: 1.0159 Val Loss: 0.9577\n",
            "Epoch: [62/1000] Step: [3140] Loss: 1.0427 Val Loss: 0.9674\n",
            "Epoch: [62/1000] Step: [3160] Loss: 1.0356 Val Loss: 0.9552\n",
            "Epoch: [63/1000] Step: [3180] Loss: 1.0546 Val Loss: 0.9521\n",
            "Epoch: [63/1000] Step: [3200] Loss: 1.0242 Val Loss: 0.9529\n",
            "Epoch: [64/1000] Step: [3220] Loss: 1.0033 Val Loss: 0.9502\n",
            "Epoch: [64/1000] Step: [3240] Loss: 1.0097 Val Loss: 0.9553\n",
            "Epoch: [64/1000] Step: [3260] Loss: 0.9819 Val Loss: 0.9532\n",
            "Epoch: [65/1000] Step: [3280] Loss: 1.0338 Val Loss: 0.9448\n",
            "Epoch: [65/1000] Step: [3300] Loss: 1.0089 Val Loss: 0.9453\n",
            "Epoch: [66/1000] Step: [3320] Loss: 0.9956 Val Loss: 0.9491\n",
            "Epoch: [66/1000] Step: [3340] Loss: 1.0131 Val Loss: 0.9555\n",
            "Epoch: [66/1000] Step: [3360] Loss: 1.0375 Val Loss: 0.9517\n",
            "Epoch: [67/1000] Step: [3380] Loss: 1.0247 Val Loss: 0.9524\n",
            "Epoch: [67/1000] Step: [3400] Loss: 1.0092 Val Loss: 0.9466\n",
            "Epoch: [68/1000] Step: [3420] Loss: 1.0323 Val Loss: 0.9477\n",
            "Epoch: [68/1000] Step: [3440] Loss: 1.0439 Val Loss: 0.9556\n",
            "Epoch: [68/1000] Step: [3460] Loss: 1.0570 Val Loss: 0.9540\n",
            "Epoch: [69/1000] Step: [3480] Loss: 1.0646 Val Loss: 0.9526\n",
            "Epoch: [69/1000] Step: [3500] Loss: 1.0341 Val Loss: 0.9499\n",
            "Epoch: [70/1000] Step: [3520] Loss: 1.0115 Val Loss: 0.9494\n",
            "Epoch: [70/1000] Step: [3540] Loss: 1.0678 Val Loss: 0.9447\n",
            "Epoch: [70/1000] Step: [3560] Loss: 1.0244 Val Loss: 0.9435\n",
            "Epoch: [71/1000] Step: [3580] Loss: 1.0276 Val Loss: 0.9492\n",
            "Epoch: [71/1000] Step: [3600] Loss: 1.0111 Val Loss: 0.9404\n",
            "Epoch: [71/1000] Step: [3620] Loss: 0.9641 Val Loss: 0.9463\n",
            "Epoch: [72/1000] Step: [3640] Loss: 1.0449 Val Loss: 0.9441\n",
            "Epoch: [72/1000] Step: [3660] Loss: 1.0057 Val Loss: 0.9421\n",
            "Epoch: [73/1000] Step: [3680] Loss: 1.0381 Val Loss: 0.9472\n",
            "Epoch: [73/1000] Step: [3700] Loss: 1.0075 Val Loss: 0.9555\n",
            "Epoch: [73/1000] Step: [3720] Loss: 1.0038 Val Loss: 0.9473\n",
            "Epoch: [74/1000] Step: [3740] Loss: 1.0340 Val Loss: 0.9520\n",
            "Epoch: [74/1000] Step: [3760] Loss: 1.0240 Val Loss: 0.9415\n",
            "Epoch: [75/1000] Step: [3780] Loss: 0.9907 Val Loss: 0.9505\n",
            "Epoch: [75/1000] Step: [3800] Loss: 0.9907 Val Loss: 0.9449\n",
            "Epoch: [75/1000] Step: [3820] Loss: 0.9880 Val Loss: 0.9412\n",
            "Epoch: [76/1000] Step: [3840] Loss: 0.9997 Val Loss: 0.9461\n",
            "Epoch: [76/1000] Step: [3860] Loss: 0.9884 Val Loss: 0.9396\n",
            "Epoch: [77/1000] Step: [3880] Loss: 0.9914 Val Loss: 0.9434\n",
            "Epoch: [77/1000] Step: [3900] Loss: 0.9877 Val Loss: 0.9418\n",
            "Epoch: [77/1000] Step: [3920] Loss: 1.0088 Val Loss: 0.9493\n",
            "Epoch: [78/1000] Step: [3940] Loss: 1.0242 Val Loss: 0.9443\n",
            "Epoch: [78/1000] Step: [3960] Loss: 0.9737 Val Loss: 0.9392\n",
            "Epoch: [79/1000] Step: [3980] Loss: 1.0125 Val Loss: 0.9362\n",
            "Epoch: [79/1000] Step: [4000] Loss: 1.0430 Val Loss: 0.9366\n",
            "Epoch: [79/1000] Step: [4020] Loss: 1.0180 Val Loss: 0.9347\n",
            "Epoch: [80/1000] Step: [4040] Loss: 1.0275 Val Loss: 0.9326\n",
            "Epoch: [80/1000] Step: [4060] Loss: 1.0033 Val Loss: 0.9369\n",
            "Epoch: [80/1000] Step: [4080] Loss: 1.1179 Val Loss: 0.9353\n",
            "Epoch: [81/1000] Step: [4100] Loss: 1.0164 Val Loss: 0.9288\n",
            "Epoch: [81/1000] Step: [4120] Loss: 0.9476 Val Loss: 0.9301\n",
            "Epoch: [82/1000] Step: [4140] Loss: 1.0060 Val Loss: 0.9331\n",
            "Epoch: [82/1000] Step: [4160] Loss: 1.0182 Val Loss: 0.9355\n",
            "Epoch: [82/1000] Step: [4180] Loss: 1.0153 Val Loss: 0.9322\n",
            "Epoch: [83/1000] Step: [4200] Loss: 1.0130 Val Loss: 0.9283\n",
            "Epoch: [83/1000] Step: [4220] Loss: 0.9967 Val Loss: 0.9279\n",
            "Epoch: [84/1000] Step: [4240] Loss: 0.9685 Val Loss: 0.9294\n",
            "Epoch: [84/1000] Step: [4260] Loss: 0.9904 Val Loss: 0.9308\n",
            "Epoch: [84/1000] Step: [4280] Loss: 0.9662 Val Loss: 0.9319\n",
            "Epoch: [85/1000] Step: [4300] Loss: 1.0167 Val Loss: 0.9286\n",
            "Epoch: [85/1000] Step: [4320] Loss: 0.9992 Val Loss: 0.9296\n",
            "Epoch: [86/1000] Step: [4340] Loss: 0.9650 Val Loss: 0.9279\n",
            "Epoch: [86/1000] Step: [4360] Loss: 0.9856 Val Loss: 0.9315\n",
            "Epoch: [86/1000] Step: [4380] Loss: 1.0043 Val Loss: 0.9302\n",
            "Epoch: [87/1000] Step: [4400] Loss: 1.0284 Val Loss: 0.9256\n",
            "Epoch: [87/1000] Step: [4420] Loss: 0.9819 Val Loss: 0.9278\n",
            "Epoch: [88/1000] Step: [4440] Loss: 0.9936 Val Loss: 0.9240\n",
            "Epoch: [88/1000] Step: [4460] Loss: 1.0046 Val Loss: 0.9255\n",
            "Epoch: [88/1000] Step: [4480] Loss: 1.0258 Val Loss: 0.9293\n",
            "Epoch: [89/1000] Step: [4500] Loss: 1.0233 Val Loss: 0.9264\n",
            "Epoch: [89/1000] Step: [4520] Loss: 0.9927 Val Loss: 0.9271\n",
            "Epoch: [90/1000] Step: [4540] Loss: 0.9970 Val Loss: 0.9227\n",
            "Epoch: [90/1000] Step: [4560] Loss: 1.0544 Val Loss: 0.9229\n",
            "Epoch: [90/1000] Step: [4580] Loss: 1.0125 Val Loss: 0.9253\n",
            "Epoch: [91/1000] Step: [4600] Loss: 1.0142 Val Loss: 0.9205\n",
            "Epoch: [91/1000] Step: [4620] Loss: 0.9746 Val Loss: 0.9240\n",
            "Epoch: [91/1000] Step: [4640] Loss: 0.9404 Val Loss: 0.9174\n",
            "Epoch: [92/1000] Step: [4660] Loss: 1.0203 Val Loss: 0.9214\n",
            "Epoch: [92/1000] Step: [4680] Loss: 1.0063 Val Loss: 0.9188\n",
            "Epoch: [93/1000] Step: [4700] Loss: 1.0108 Val Loss: 0.9174\n",
            "Epoch: [93/1000] Step: [4720] Loss: 0.9800 Val Loss: 0.9278\n",
            "Epoch: [93/1000] Step: [4740] Loss: 0.9995 Val Loss: 0.9134\n",
            "Epoch: [94/1000] Step: [4760] Loss: 1.0174 Val Loss: 0.9178\n",
            "Epoch: [94/1000] Step: [4780] Loss: 0.9956 Val Loss: 0.9163\n",
            "Epoch: [95/1000] Step: [4800] Loss: 0.9602 Val Loss: 0.9184\n",
            "Epoch: [95/1000] Step: [4820] Loss: 0.9654 Val Loss: 0.9219\n",
            "Epoch: [95/1000] Step: [4840] Loss: 0.9701 Val Loss: 0.9249\n",
            "Epoch: [96/1000] Step: [4860] Loss: 0.9911 Val Loss: 0.9204\n",
            "Epoch: [96/1000] Step: [4880] Loss: 0.9798 Val Loss: 0.9153\n",
            "Epoch: [97/1000] Step: [4900] Loss: 0.9838 Val Loss: 0.9197\n",
            "Epoch: [97/1000] Step: [4920] Loss: 0.9686 Val Loss: 0.9194\n",
            "Epoch: [97/1000] Step: [4940] Loss: 1.0136 Val Loss: 0.9214\n",
            "Epoch: [98/1000] Step: [4960] Loss: 1.0096 Val Loss: 0.9211\n",
            "Epoch: [98/1000] Step: [4980] Loss: 0.9617 Val Loss: 0.9238\n",
            "Epoch: [99/1000] Step: [5000] Loss: 0.9793 Val Loss: 0.9169\n",
            "Epoch: [99/1000] Step: [5020] Loss: 1.0275 Val Loss: 0.9156\n",
            "Epoch: [99/1000] Step: [5040] Loss: 0.9903 Val Loss: 0.9141\n",
            "Epoch: [100/1000] Step: [5060] Loss: 1.0143 Val Loss: 0.9178\n",
            "Epoch: [100/1000] Step: [5080] Loss: 0.9996 Val Loss: 0.9121\n",
            "Epoch: [100/1000] Step: [5100] Loss: 1.1238 Val Loss: 0.9184\n",
            "Epoch: [101/1000] Step: [5120] Loss: 0.9862 Val Loss: 0.9107\n",
            "Epoch: [101/1000] Step: [5140] Loss: 0.9431 Val Loss: 0.9091\n",
            "Epoch: [102/1000] Step: [5160] Loss: 0.9946 Val Loss: 0.9095\n",
            "Epoch: [102/1000] Step: [5180] Loss: 0.9984 Val Loss: 0.9108\n",
            "Epoch: [102/1000] Step: [5200] Loss: 0.9906 Val Loss: 0.9174\n",
            "Epoch: [103/1000] Step: [5220] Loss: 1.0209 Val Loss: 0.9184\n",
            "Epoch: [103/1000] Step: [5240] Loss: 0.9980 Val Loss: 0.9132\n",
            "Epoch: [104/1000] Step: [5260] Loss: 0.9707 Val Loss: 0.9163\n",
            "Epoch: [104/1000] Step: [5280] Loss: 0.9727 Val Loss: 0.9115\n",
            "Epoch: [104/1000] Step: [5300] Loss: 0.9403 Val Loss: 0.9082\n",
            "Epoch: [105/1000] Step: [5320] Loss: 0.9811 Val Loss: 0.9067\n",
            "Epoch: [105/1000] Step: [5340] Loss: 0.9760 Val Loss: 0.9080\n",
            "Epoch: [106/1000] Step: [5360] Loss: 0.9665 Val Loss: 0.9124\n",
            "Epoch: [106/1000] Step: [5380] Loss: 0.9812 Val Loss: 0.9167\n",
            "Epoch: [106/1000] Step: [5400] Loss: 0.9946 Val Loss: 0.9147\n",
            "Epoch: [107/1000] Step: [5420] Loss: 0.9896 Val Loss: 0.9071\n",
            "Epoch: [107/1000] Step: [5440] Loss: 0.9652 Val Loss: 0.9076\n",
            "Epoch: [108/1000] Step: [5460] Loss: 0.9884 Val Loss: 0.9115\n",
            "Epoch: [108/1000] Step: [5480] Loss: 0.9774 Val Loss: 0.9156\n",
            "Epoch: [108/1000] Step: [5500] Loss: 1.0072 Val Loss: 0.9105\n",
            "Epoch: [109/1000] Step: [5520] Loss: 1.0147 Val Loss: 0.9156\n",
            "Epoch: [109/1000] Step: [5540] Loss: 0.9730 Val Loss: 0.9098\n",
            "Epoch: [110/1000] Step: [5560] Loss: 0.9939 Val Loss: 0.9108\n",
            "Epoch: [110/1000] Step: [5580] Loss: 1.0193 Val Loss: 0.9136\n",
            "Epoch: [110/1000] Step: [5600] Loss: 0.9823 Val Loss: 0.9076\n",
            "Epoch: [111/1000] Step: [5620] Loss: 0.9933 Val Loss: 0.9050\n",
            "Epoch: [111/1000] Step: [5640] Loss: 0.9756 Val Loss: 0.9189\n",
            "Epoch: [111/1000] Step: [5660] Loss: 0.9434 Val Loss: 0.9100\n",
            "Epoch: [112/1000] Step: [5680] Loss: 1.0292 Val Loss: 0.9068\n",
            "Epoch: [112/1000] Step: [5700] Loss: 0.9888 Val Loss: 0.9078\n",
            "Epoch: [113/1000] Step: [5720] Loss: 0.9939 Val Loss: 0.9050\n",
            "Epoch: [113/1000] Step: [5740] Loss: 0.9475 Val Loss: 0.9136\n",
            "Epoch: [113/1000] Step: [5760] Loss: 0.9805 Val Loss: 0.9108\n",
            "Epoch: [114/1000] Step: [5780] Loss: 1.0147 Val Loss: 0.9069\n",
            "Epoch: [114/1000] Step: [5800] Loss: 0.9919 Val Loss: 0.9044\n",
            "Epoch: [115/1000] Step: [5820] Loss: 0.9496 Val Loss: 0.9013\n",
            "Epoch: [115/1000] Step: [5840] Loss: 0.9429 Val Loss: 0.9081\n",
            "Epoch: [115/1000] Step: [5860] Loss: 0.9515 Val Loss: 0.9059\n",
            "Epoch: [116/1000] Step: [5880] Loss: 0.9775 Val Loss: 0.9123\n",
            "Epoch: [116/1000] Step: [5900] Loss: 0.9781 Val Loss: 0.9095\n",
            "Epoch: [117/1000] Step: [5920] Loss: 0.9564 Val Loss: 0.9095\n",
            "Epoch: [117/1000] Step: [5940] Loss: 0.9641 Val Loss: 0.9073\n",
            "Epoch: [117/1000] Step: [5960] Loss: 0.9717 Val Loss: 0.9048\n",
            "Epoch: [118/1000] Step: [5980] Loss: 0.9913 Val Loss: 0.9028\n",
            "Epoch: [118/1000] Step: [6000] Loss: 0.9510 Val Loss: 0.9014\n",
            "Epoch: [119/1000] Step: [6020] Loss: 0.9741 Val Loss: 0.9032\n",
            "Epoch: [119/1000] Step: [6040] Loss: 1.0145 Val Loss: 0.9058\n",
            "Epoch: [119/1000] Step: [6060] Loss: 0.9778 Val Loss: 0.9028\n",
            "Epoch: [120/1000] Step: [6080] Loss: 0.9991 Val Loss: 0.9055\n",
            "Epoch: [120/1000] Step: [6100] Loss: 0.9683 Val Loss: 0.9042\n",
            "Epoch: [120/1000] Step: [6120] Loss: 1.1213 Val Loss: 0.8994\n",
            "Epoch: [121/1000] Step: [6140] Loss: 0.9973 Val Loss: 0.9023\n",
            "Epoch: [121/1000] Step: [6160] Loss: 0.9267 Val Loss: 0.8984\n",
            "Epoch: [122/1000] Step: [6180] Loss: 0.9596 Val Loss: 0.8990\n",
            "Epoch: [122/1000] Step: [6200] Loss: 0.9655 Val Loss: 0.9021\n",
            "Epoch: [122/1000] Step: [6220] Loss: 0.9678 Val Loss: 0.8949\n",
            "Epoch: [123/1000] Step: [6240] Loss: 0.9848 Val Loss: 0.9016\n",
            "Epoch: [123/1000] Step: [6260] Loss: 0.9835 Val Loss: 0.8932\n",
            "Epoch: [124/1000] Step: [6280] Loss: 0.9469 Val Loss: 0.8996\n",
            "Epoch: [124/1000] Step: [6300] Loss: 0.9828 Val Loss: 0.9054\n",
            "Epoch: [124/1000] Step: [6320] Loss: 0.9281 Val Loss: 0.8997\n",
            "Epoch: [125/1000] Step: [6340] Loss: 0.9912 Val Loss: 0.8959\n",
            "Epoch: [125/1000] Step: [6360] Loss: 0.9597 Val Loss: 0.8955\n",
            "Epoch: [126/1000] Step: [6380] Loss: 0.9483 Val Loss: 0.8926\n",
            "Epoch: [126/1000] Step: [6400] Loss: 0.9626 Val Loss: 0.8992\n",
            "Epoch: [126/1000] Step: [6420] Loss: 0.9682 Val Loss: 0.8980\n",
            "Epoch: [127/1000] Step: [6440] Loss: 0.9662 Val Loss: 0.9068\n",
            "Epoch: [127/1000] Step: [6460] Loss: 0.9548 Val Loss: 0.8987\n",
            "Epoch: [128/1000] Step: [6480] Loss: 0.9693 Val Loss: 0.9018\n",
            "Epoch: [128/1000] Step: [6500] Loss: 0.9836 Val Loss: 0.9057\n",
            "Epoch: [128/1000] Step: [6520] Loss: 0.9883 Val Loss: 0.8973\n",
            "Epoch: [129/1000] Step: [6540] Loss: 0.9951 Val Loss: 0.9047\n",
            "Epoch: [129/1000] Step: [6560] Loss: 0.9688 Val Loss: 0.9078\n",
            "Epoch: [130/1000] Step: [6580] Loss: 0.9648 Val Loss: 0.8988\n",
            "Epoch: [130/1000] Step: [6600] Loss: 1.0345 Val Loss: 0.8958\n",
            "Epoch: [130/1000] Step: [6620] Loss: 0.9547 Val Loss: 0.8993\n",
            "Epoch: [131/1000] Step: [6640] Loss: 0.9761 Val Loss: 0.8994\n",
            "Epoch: [131/1000] Step: [6660] Loss: 0.9515 Val Loss: 0.9056\n",
            "Epoch: [131/1000] Step: [6680] Loss: 0.9092 Val Loss: 0.8967\n",
            "Epoch: [132/1000] Step: [6700] Loss: 1.0092 Val Loss: 0.9024\n",
            "Epoch: [132/1000] Step: [6720] Loss: 0.9848 Val Loss: 0.8998\n",
            "Epoch: [133/1000] Step: [6740] Loss: 0.9827 Val Loss: 0.8986\n",
            "Epoch: [133/1000] Step: [6760] Loss: 0.9430 Val Loss: 0.8994\n",
            "Epoch: [133/1000] Step: [6780] Loss: 0.9888 Val Loss: 0.8959\n",
            "Epoch: [134/1000] Step: [6800] Loss: 0.9982 Val Loss: 0.8974\n",
            "Epoch: [134/1000] Step: [6820] Loss: 0.9829 Val Loss: 0.8911\n",
            "Epoch: [135/1000] Step: [6840] Loss: 0.9305 Val Loss: 0.8894\n",
            "Epoch: [135/1000] Step: [6860] Loss: 0.9312 Val Loss: 0.8940\n",
            "Epoch: [135/1000] Step: [6880] Loss: 0.9378 Val Loss: 0.8953\n",
            "Epoch: [136/1000] Step: [6900] Loss: 0.9630 Val Loss: 0.8955\n",
            "Epoch: [136/1000] Step: [6920] Loss: 0.9579 Val Loss: 0.8928\n",
            "Epoch: [137/1000] Step: [6940] Loss: 0.9701 Val Loss: 0.8919\n",
            "Epoch: [137/1000] Step: [6960] Loss: 0.9329 Val Loss: 0.8984\n",
            "Epoch: [137/1000] Step: [6980] Loss: 0.9762 Val Loss: 0.8962\n",
            "Epoch: [138/1000] Step: [7000] Loss: 0.9858 Val Loss: 0.9000\n",
            "Epoch: [138/1000] Step: [7020] Loss: 0.9582 Val Loss: 0.8918\n",
            "Epoch: [139/1000] Step: [7040] Loss: 0.9646 Val Loss: 0.8960\n",
            "Epoch: [139/1000] Step: [7060] Loss: 1.0247 Val Loss: 0.8943\n",
            "Epoch: [139/1000] Step: [7080] Loss: 0.9693 Val Loss: 0.8950\n",
            "Epoch: [140/1000] Step: [7100] Loss: 0.9885 Val Loss: 0.8936\n",
            "Epoch: [140/1000] Step: [7120] Loss: 0.9626 Val Loss: 0.8991\n",
            "Epoch: [140/1000] Step: [7140] Loss: 1.0970 Val Loss: 0.8975\n",
            "Epoch: [141/1000] Step: [7160] Loss: 0.9738 Val Loss: 0.8967\n",
            "Epoch: [141/1000] Step: [7180] Loss: 0.9029 Val Loss: 0.8899\n",
            "Epoch: [142/1000] Step: [7200] Loss: 0.9600 Val Loss: 0.8870\n",
            "Epoch: [142/1000] Step: [7220] Loss: 0.9773 Val Loss: 0.8953\n",
            "Epoch: [142/1000] Step: [7240] Loss: 0.9613 Val Loss: 0.8944\n",
            "Epoch: [143/1000] Step: [7260] Loss: 0.9888 Val Loss: 0.8920\n",
            "Epoch: [143/1000] Step: [7280] Loss: 0.9721 Val Loss: 0.8912\n",
            "Epoch: [144/1000] Step: [7300] Loss: 0.9478 Val Loss: 0.8874\n",
            "Epoch: [144/1000] Step: [7320] Loss: 0.9462 Val Loss: 0.8941\n",
            "Epoch: [144/1000] Step: [7340] Loss: 0.9112 Val Loss: 0.8999\n",
            "Epoch: [145/1000] Step: [7360] Loss: 0.9658 Val Loss: 0.8859\n",
            "Epoch: [145/1000] Step: [7380] Loss: 0.9498 Val Loss: 0.8930\n",
            "Epoch: [146/1000] Step: [7400] Loss: 0.9477 Val Loss: 0.8972\n",
            "Epoch: [146/1000] Step: [7420] Loss: 0.9564 Val Loss: 0.8933\n",
            "Epoch: [146/1000] Step: [7440] Loss: 0.9772 Val Loss: 0.8921\n",
            "Epoch: [147/1000] Step: [7460] Loss: 0.9635 Val Loss: 0.8899\n",
            "Epoch: [147/1000] Step: [7480] Loss: 0.9538 Val Loss: 0.8918\n",
            "Epoch: [148/1000] Step: [7500] Loss: 0.9527 Val Loss: 0.8880\n",
            "Epoch: [148/1000] Step: [7520] Loss: 0.9690 Val Loss: 0.8874\n",
            "Epoch: [148/1000] Step: [7540] Loss: 0.9833 Val Loss: 0.8884\n",
            "Epoch: [149/1000] Step: [7560] Loss: 1.0026 Val Loss: 0.8881\n",
            "Epoch: [149/1000] Step: [7580] Loss: 0.9782 Val Loss: 0.8884\n",
            "Epoch: [150/1000] Step: [7600] Loss: 0.9461 Val Loss: 0.8906\n",
            "Epoch: [150/1000] Step: [7620] Loss: 0.9956 Val Loss: 0.8836\n",
            "Epoch: [150/1000] Step: [7640] Loss: 0.9528 Val Loss: 0.8871\n",
            "Epoch: [151/1000] Step: [7660] Loss: 0.9861 Val Loss: 0.8826\n",
            "Epoch: [151/1000] Step: [7680] Loss: 0.9373 Val Loss: 0.8878\n",
            "Epoch: [151/1000] Step: [7700] Loss: 0.9044 Val Loss: 0.8833\n",
            "Epoch: [152/1000] Step: [7720] Loss: 1.0052 Val Loss: 0.8984\n",
            "Epoch: [152/1000] Step: [7740] Loss: 0.9624 Val Loss: 0.8909\n",
            "Epoch: [153/1000] Step: [7760] Loss: 0.9699 Val Loss: 0.8913\n",
            "Epoch: [153/1000] Step: [7780] Loss: 0.9471 Val Loss: 0.8903\n",
            "Epoch: [153/1000] Step: [7800] Loss: 0.9602 Val Loss: 0.8898\n",
            "Epoch: [154/1000] Step: [7820] Loss: 0.9841 Val Loss: 0.8934\n",
            "Epoch: [154/1000] Step: [7840] Loss: 0.9595 Val Loss: 0.8988\n",
            "Epoch: [155/1000] Step: [7860] Loss: 0.9294 Val Loss: 0.8837\n",
            "Epoch: [155/1000] Step: [7880] Loss: 0.9176 Val Loss: 0.8864\n",
            "Epoch: [155/1000] Step: [7900] Loss: 0.9348 Val Loss: 0.8905\n",
            "Epoch: [156/1000] Step: [7920] Loss: 0.9405 Val Loss: 0.8901\n",
            "Epoch: [156/1000] Step: [7940] Loss: 0.9457 Val Loss: 0.8912\n",
            "Epoch: [157/1000] Step: [7960] Loss: 0.9422 Val Loss: 0.8934\n",
            "Epoch: [157/1000] Step: [7980] Loss: 0.9573 Val Loss: 0.8978\n",
            "Epoch: [157/1000] Step: [8000] Loss: 0.9785 Val Loss: 0.8927\n",
            "Epoch: [158/1000] Step: [8020] Loss: 0.9610 Val Loss: 0.8983\n",
            "Epoch: [158/1000] Step: [8040] Loss: 0.9348 Val Loss: 0.8940\n",
            "Epoch: [159/1000] Step: [8060] Loss: 0.9702 Val Loss: 0.8887\n",
            "Epoch: [159/1000] Step: [8080] Loss: 0.9903 Val Loss: 0.8902\n",
            "Epoch: [159/1000] Step: [8100] Loss: 0.9502 Val Loss: 0.8876\n",
            "Epoch: [160/1000] Step: [8120] Loss: 0.9672 Val Loss: 0.8897\n",
            "Epoch: [160/1000] Step: [8140] Loss: 0.9468 Val Loss: 0.8899\n",
            "Epoch: [160/1000] Step: [8160] Loss: 1.0886 Val Loss: 0.8853\n",
            "Epoch: [161/1000] Step: [8180] Loss: 0.9892 Val Loss: 0.8905\n",
            "Epoch: [161/1000] Step: [8200] Loss: 0.9058 Val Loss: 0.8878\n",
            "Epoch: [162/1000] Step: [8220] Loss: 0.9589 Val Loss: 0.8928\n",
            "Epoch: [162/1000] Step: [8240] Loss: 0.9555 Val Loss: 0.8895\n",
            "Epoch: [162/1000] Step: [8260] Loss: 0.9453 Val Loss: 0.8851\n",
            "Epoch: [163/1000] Step: [8280] Loss: 0.9870 Val Loss: 0.8913\n",
            "Epoch: [163/1000] Step: [8300] Loss: 0.9621 Val Loss: 0.8837\n",
            "Epoch: [164/1000] Step: [8320] Loss: 0.9342 Val Loss: 0.8940\n",
            "Epoch: [164/1000] Step: [8340] Loss: 0.9591 Val Loss: 0.8907\n",
            "Epoch: [164/1000] Step: [8360] Loss: 0.9344 Val Loss: 0.8924\n",
            "Epoch: [165/1000] Step: [8380] Loss: 0.9808 Val Loss: 0.8926\n",
            "Epoch: [165/1000] Step: [8400] Loss: 0.9430 Val Loss: 0.8848\n",
            "Epoch: [166/1000] Step: [8420] Loss: 0.9273 Val Loss: 0.8930\n",
            "Epoch: [166/1000] Step: [8440] Loss: 0.9370 Val Loss: 0.8909\n",
            "Epoch: [166/1000] Step: [8460] Loss: 0.9578 Val Loss: 0.8901\n",
            "Epoch: [167/1000] Step: [8480] Loss: 0.9523 Val Loss: 0.8870\n",
            "Epoch: [167/1000] Step: [8500] Loss: 0.9154 Val Loss: 0.8857\n",
            "Epoch: [168/1000] Step: [8520] Loss: 0.9539 Val Loss: 0.8900\n",
            "Epoch: [168/1000] Step: [8540] Loss: 0.9775 Val Loss: 0.8872\n",
            "Epoch: [168/1000] Step: [8560] Loss: 0.9911 Val Loss: 0.8847\n",
            "Epoch: [169/1000] Step: [8580] Loss: 0.9874 Val Loss: 0.8802\n",
            "Epoch: [169/1000] Step: [8600] Loss: 0.9559 Val Loss: 0.8847\n",
            "Epoch: [170/1000] Step: [8620] Loss: 0.9321 Val Loss: 0.8833\n",
            "Epoch: [170/1000] Step: [8640] Loss: 0.9842 Val Loss: 0.8791\n",
            "Epoch: [170/1000] Step: [8660] Loss: 0.9549 Val Loss: 0.8777\n",
            "Epoch: [171/1000] Step: [8680] Loss: 0.9791 Val Loss: 0.8817\n",
            "Epoch: [171/1000] Step: [8700] Loss: 0.9377 Val Loss: 0.8846\n",
            "Epoch: [171/1000] Step: [8720] Loss: 0.9032 Val Loss: 0.8837\n",
            "Epoch: [172/1000] Step: [8740] Loss: 0.9925 Val Loss: 0.8858\n",
            "Epoch: [172/1000] Step: [8760] Loss: 0.9558 Val Loss: 0.8784\n",
            "Epoch: [173/1000] Step: [8780] Loss: 0.9739 Val Loss: 0.8891\n",
            "Epoch: [173/1000] Step: [8800] Loss: 0.9348 Val Loss: 0.8889\n",
            "Epoch: [173/1000] Step: [8820] Loss: 0.9565 Val Loss: 0.8818\n",
            "Epoch: [174/1000] Step: [8840] Loss: 0.9671 Val Loss: 0.8798\n",
            "Epoch: [174/1000] Step: [8860] Loss: 0.9458 Val Loss: 0.8821\n",
            "Epoch: [175/1000] Step: [8880] Loss: 0.9308 Val Loss: 0.8965\n",
            "Epoch: [175/1000] Step: [8900] Loss: 0.9217 Val Loss: 0.8909\n",
            "Epoch: [175/1000] Step: [8920] Loss: 0.9452 Val Loss: 0.8898\n",
            "Epoch: [176/1000] Step: [8940] Loss: 0.9523 Val Loss: 0.8994\n",
            "Epoch: [176/1000] Step: [8960] Loss: 0.9552 Val Loss: 0.8938\n",
            "Epoch: [177/1000] Step: [8980] Loss: 0.9503 Val Loss: 0.8911\n",
            "Epoch: [177/1000] Step: [9000] Loss: 0.9267 Val Loss: 0.8840\n",
            "Epoch: [177/1000] Step: [9020] Loss: 0.9398 Val Loss: 0.8785\n",
            "Epoch: [178/1000] Step: [9040] Loss: 0.9689 Val Loss: 0.8852\n",
            "Epoch: [178/1000] Step: [9060] Loss: 0.9473 Val Loss: 0.8877\n",
            "Epoch: [179/1000] Step: [9080] Loss: 0.9635 Val Loss: 0.8840\n",
            "Epoch: [179/1000] Step: [9100] Loss: 0.9882 Val Loss: 0.8852\n",
            "Epoch: [179/1000] Step: [9120] Loss: 0.9677 Val Loss: 0.8838\n",
            "Epoch: [180/1000] Step: [9140] Loss: 0.9729 Val Loss: 0.8811\n",
            "Epoch: [180/1000] Step: [9160] Loss: 0.9175 Val Loss: 0.8907\n",
            "Epoch: [180/1000] Step: [9180] Loss: 1.0760 Val Loss: 0.8821\n",
            "Epoch: [181/1000] Step: [9200] Loss: 0.9664 Val Loss: 0.8838\n",
            "Epoch: [181/1000] Step: [9220] Loss: 0.9134 Val Loss: 0.8845\n",
            "Epoch: [182/1000] Step: [9240] Loss: 0.9638 Val Loss: 0.8832\n",
            "Epoch: [182/1000] Step: [9260] Loss: 0.9476 Val Loss: 0.8770\n",
            "Epoch: [182/1000] Step: [9280] Loss: 0.9509 Val Loss: 0.8771\n",
            "Epoch: [183/1000] Step: [9300] Loss: 0.9616 Val Loss: 0.8860\n",
            "Epoch: [183/1000] Step: [9320] Loss: 0.9818 Val Loss: 0.8816\n",
            "Epoch: [184/1000] Step: [9340] Loss: 0.9325 Val Loss: 0.8837\n",
            "Epoch: [184/1000] Step: [9360] Loss: 0.9477 Val Loss: 0.8783\n",
            "Epoch: [184/1000] Step: [9380] Loss: 0.9176 Val Loss: 0.8772\n",
            "Epoch: [185/1000] Step: [9400] Loss: 0.9576 Val Loss: 0.8768\n",
            "Epoch: [185/1000] Step: [9420] Loss: 0.9506 Val Loss: 0.8836\n",
            "Epoch: [186/1000] Step: [9440] Loss: 0.9159 Val Loss: 0.8787\n",
            "Epoch: [186/1000] Step: [9460] Loss: 0.9359 Val Loss: 0.8812\n",
            "Epoch: [186/1000] Step: [9480] Loss: 0.9476 Val Loss: 0.8801\n",
            "Epoch: [187/1000] Step: [9500] Loss: 0.9523 Val Loss: 0.8781\n",
            "Epoch: [187/1000] Step: [9520] Loss: 0.9179 Val Loss: 0.8856\n",
            "Epoch: [188/1000] Step: [9540] Loss: 0.9206 Val Loss: 0.8782\n",
            "Epoch: [188/1000] Step: [9560] Loss: 0.9752 Val Loss: 0.8766\n",
            "Epoch: [188/1000] Step: [9580] Loss: 0.9650 Val Loss: 0.8737\n",
            "Epoch: [189/1000] Step: [9600] Loss: 0.9909 Val Loss: 0.8756\n",
            "Epoch: [189/1000] Step: [9620] Loss: 0.9566 Val Loss: 0.8734\n",
            "Epoch: [190/1000] Step: [9640] Loss: 0.9499 Val Loss: 0.8787\n",
            "Epoch: [190/1000] Step: [9660] Loss: 1.0023 Val Loss: 0.8805\n",
            "Epoch: [190/1000] Step: [9680] Loss: 0.9326 Val Loss: 0.8744\n",
            "Epoch: [191/1000] Step: [9700] Loss: 0.9677 Val Loss: 0.8802\n",
            "Epoch: [191/1000] Step: [9720] Loss: 0.9550 Val Loss: 0.8760\n",
            "Epoch: [191/1000] Step: [9740] Loss: 0.8784 Val Loss: 0.8756\n",
            "Epoch: [192/1000] Step: [9760] Loss: 0.9811 Val Loss: 0.8731\n",
            "Epoch: [192/1000] Step: [9780] Loss: 0.9521 Val Loss: 0.8723\n",
            "Epoch: [193/1000] Step: [9800] Loss: 0.9782 Val Loss: 0.8790\n",
            "Epoch: [193/1000] Step: [9820] Loss: 0.9207 Val Loss: 0.8790\n",
            "Epoch: [193/1000] Step: [9840] Loss: 0.9358 Val Loss: 0.8787\n",
            "Epoch: [194/1000] Step: [9860] Loss: 0.9612 Val Loss: 0.8791\n",
            "Epoch: [194/1000] Step: [9880] Loss: 0.9427 Val Loss: 0.8804\n",
            "Epoch: [195/1000] Step: [9900] Loss: 0.8970 Val Loss: 0.8756\n",
            "Epoch: [195/1000] Step: [9920] Loss: 0.9046 Val Loss: 0.8850\n",
            "Epoch: [195/1000] Step: [9940] Loss: 0.9248 Val Loss: 0.8805\n",
            "Epoch: [196/1000] Step: [9960] Loss: 0.9413 Val Loss: 0.8796\n",
            "Epoch: [196/1000] Step: [9980] Loss: 0.9439 Val Loss: 0.8841\n",
            "Epoch: [197/1000] Step: [10000] Loss: 0.9338 Val Loss: 0.8779\n",
            "Epoch: [197/1000] Step: [10020] Loss: 0.9291 Val Loss: 0.8709\n",
            "Epoch: [197/1000] Step: [10040] Loss: 0.9516 Val Loss: 0.8769\n",
            "Epoch: [198/1000] Step: [10060] Loss: 0.9599 Val Loss: 0.8901\n",
            "Epoch: [198/1000] Step: [10080] Loss: 0.9265 Val Loss: 0.8796\n",
            "Epoch: [199/1000] Step: [10100] Loss: 0.9233 Val Loss: 0.8822\n",
            "Epoch: [199/1000] Step: [10120] Loss: 0.9960 Val Loss: 0.8780\n",
            "Epoch: [199/1000] Step: [10140] Loss: 0.9533 Val Loss: 0.8798\n",
            "Epoch: [200/1000] Step: [10160] Loss: 0.9766 Val Loss: 0.8813\n",
            "Epoch: [200/1000] Step: [10180] Loss: 0.9451 Val Loss: 0.8834\n",
            "Epoch: [200/1000] Step: [10200] Loss: 1.0638 Val Loss: 0.8761\n",
            "Epoch: [201/1000] Step: [10220] Loss: 0.9647 Val Loss: 0.8772\n",
            "Epoch: [201/1000] Step: [10240] Loss: 0.8878 Val Loss: 0.8746\n",
            "Epoch: [202/1000] Step: [10260] Loss: 0.9571 Val Loss: 0.8819\n",
            "Epoch: [202/1000] Step: [10280] Loss: 0.9575 Val Loss: 0.8903\n",
            "Epoch: [202/1000] Step: [10300] Loss: 0.9548 Val Loss: 0.8841\n",
            "Epoch: [203/1000] Step: [10320] Loss: 0.9702 Val Loss: 0.8828\n",
            "Epoch: [203/1000] Step: [10340] Loss: 0.9756 Val Loss: 0.8791\n",
            "Epoch: [204/1000] Step: [10360] Loss: 0.9341 Val Loss: 0.8831\n",
            "Epoch: [204/1000] Step: [10380] Loss: 0.9533 Val Loss: 0.8717\n",
            "Epoch: [204/1000] Step: [10400] Loss: 0.9206 Val Loss: 0.8761\n",
            "Epoch: [205/1000] Step: [10420] Loss: 0.9683 Val Loss: 0.8753\n",
            "Epoch: [205/1000] Step: [10440] Loss: 0.9506 Val Loss: 0.8767\n",
            "Epoch: [206/1000] Step: [10460] Loss: 0.9144 Val Loss: 0.8840\n",
            "Epoch: [206/1000] Step: [10480] Loss: 0.9054 Val Loss: 0.8809\n",
            "Epoch: [206/1000] Step: [10500] Loss: 0.9631 Val Loss: 0.8770\n",
            "Epoch: [207/1000] Step: [10520] Loss: 0.9422 Val Loss: 0.8743\n",
            "Epoch: [207/1000] Step: [10540] Loss: 0.9268 Val Loss: 0.8760\n",
            "Epoch: [208/1000] Step: [10560] Loss: 0.9298 Val Loss: 0.8773\n",
            "Epoch: [208/1000] Step: [10580] Loss: 0.9527 Val Loss: 0.8785\n",
            "Epoch: [208/1000] Step: [10600] Loss: 0.9612 Val Loss: 0.8716\n",
            "Epoch: [209/1000] Step: [10620] Loss: 0.9859 Val Loss: 0.8751\n",
            "Epoch: [209/1000] Step: [10640] Loss: 0.9485 Val Loss: 0.8769\n",
            "Epoch: [210/1000] Step: [10660] Loss: 0.9122 Val Loss: 0.8728\n",
            "Epoch: [210/1000] Step: [10680] Loss: 0.9807 Val Loss: 0.8778\n",
            "Epoch: [210/1000] Step: [10700] Loss: 0.9451 Val Loss: 0.8739\n",
            "Epoch: [211/1000] Step: [10720] Loss: 0.9644 Val Loss: 0.8747\n",
            "Epoch: [211/1000] Step: [10740] Loss: 0.9402 Val Loss: 0.8745\n",
            "Epoch: [211/1000] Step: [10760] Loss: 0.8926 Val Loss: 0.8705\n",
            "Epoch: [212/1000] Step: [10780] Loss: 0.9755 Val Loss: 0.8773\n",
            "Epoch: [212/1000] Step: [10800] Loss: 0.9359 Val Loss: 0.8724\n",
            "Epoch: [213/1000] Step: [10820] Loss: 0.9703 Val Loss: 0.8740\n",
            "Epoch: [213/1000] Step: [10840] Loss: 0.9165 Val Loss: 0.8750\n",
            "Epoch: [213/1000] Step: [10860] Loss: 0.9394 Val Loss: 0.8720\n",
            "Epoch: [214/1000] Step: [10880] Loss: 0.9726 Val Loss: 0.8743\n",
            "Epoch: [214/1000] Step: [10900] Loss: 0.9615 Val Loss: 0.8731\n",
            "Epoch: [215/1000] Step: [10920] Loss: 0.8882 Val Loss: 0.8702\n",
            "Epoch: [215/1000] Step: [10940] Loss: 0.8981 Val Loss: 0.8750\n",
            "Epoch: [215/1000] Step: [10960] Loss: 0.9296 Val Loss: 0.8761\n",
            "Epoch: [216/1000] Step: [10980] Loss: 0.9512 Val Loss: 0.8714\n",
            "Epoch: [216/1000] Step: [11000] Loss: 0.9240 Val Loss: 0.8720\n",
            "Epoch: [217/1000] Step: [11020] Loss: 0.9196 Val Loss: 0.8751\n",
            "Epoch: [217/1000] Step: [11040] Loss: 0.9239 Val Loss: 0.8714\n",
            "Epoch: [217/1000] Step: [11060] Loss: 0.9564 Val Loss: 0.8645\n",
            "Epoch: [218/1000] Step: [11080] Loss: 0.9383 Val Loss: 0.8701\n",
            "Epoch: [218/1000] Step: [11100] Loss: 0.9093 Val Loss: 0.8661\n",
            "Epoch: [219/1000] Step: [11120] Loss: 0.9281 Val Loss: 0.8688\n",
            "Epoch: [219/1000] Step: [11140] Loss: 0.9842 Val Loss: 0.8702\n",
            "Epoch: [219/1000] Step: [11160] Loss: 0.9306 Val Loss: 0.8687\n",
            "Epoch: [220/1000] Step: [11180] Loss: 0.9512 Val Loss: 0.8682\n",
            "Epoch: [220/1000] Step: [11200] Loss: 0.9184 Val Loss: 0.8761\n",
            "Epoch: [220/1000] Step: [11220] Loss: 1.0571 Val Loss: 0.8708\n",
            "Epoch: [221/1000] Step: [11240] Loss: 0.9657 Val Loss: 0.8713\n",
            "Epoch: [221/1000] Step: [11260] Loss: 0.8693 Val Loss: 0.8748\n",
            "Epoch: [222/1000] Step: [11280] Loss: 0.9431 Val Loss: 0.8755\n",
            "Epoch: [222/1000] Step: [11300] Loss: 0.9427 Val Loss: 0.8680\n",
            "Epoch: [222/1000] Step: [11320] Loss: 0.9429 Val Loss: 0.8731\n",
            "Epoch: [223/1000] Step: [11340] Loss: 0.9684 Val Loss: 0.8799\n",
            "Epoch: [223/1000] Step: [11360] Loss: 0.9555 Val Loss: 0.8813\n",
            "Epoch: [224/1000] Step: [11380] Loss: 0.9059 Val Loss: 0.8797\n",
            "Epoch: [224/1000] Step: [11400] Loss: 0.9707 Val Loss: 0.8838\n",
            "Epoch: [224/1000] Step: [11420] Loss: 0.8930 Val Loss: 0.8761\n",
            "Epoch: [225/1000] Step: [11440] Loss: 0.9919 Val Loss: 0.8780\n",
            "Epoch: [225/1000] Step: [11460] Loss: 0.9395 Val Loss: 0.8724\n",
            "Epoch: [226/1000] Step: [11480] Loss: 0.9063 Val Loss: 0.8690\n",
            "Epoch: [226/1000] Step: [11500] Loss: 0.9224 Val Loss: 0.8759\n",
            "Epoch: [226/1000] Step: [11520] Loss: 0.9874 Val Loss: 0.9026\n",
            "Epoch: [227/1000] Step: [11540] Loss: 0.9898 Val Loss: 0.9045\n",
            "Epoch: [227/1000] Step: [11560] Loss: 0.9301 Val Loss: 0.8930\n",
            "Epoch: [228/1000] Step: [11580] Loss: 0.9542 Val Loss: 0.8757\n",
            "Epoch: [228/1000] Step: [11600] Loss: 0.9796 Val Loss: 0.8852\n",
            "Epoch: [228/1000] Step: [11620] Loss: 0.9782 Val Loss: 0.8815\n",
            "Epoch: [229/1000] Step: [11640] Loss: 0.9865 Val Loss: 0.8759\n",
            "Epoch: [229/1000] Step: [11660] Loss: 0.9742 Val Loss: 0.8815\n",
            "Epoch: [230/1000] Step: [11680] Loss: 0.9392 Val Loss: 0.8704\n",
            "Epoch: [230/1000] Step: [11700] Loss: 0.9938 Val Loss: 0.8765\n",
            "Epoch: [230/1000] Step: [11720] Loss: 0.9361 Val Loss: 0.8723\n",
            "Epoch: [231/1000] Step: [11740] Loss: 0.9385 Val Loss: 0.8767\n",
            "Epoch: [231/1000] Step: [11760] Loss: 0.9249 Val Loss: 0.8754\n",
            "Epoch: [231/1000] Step: [11780] Loss: 0.8902 Val Loss: 0.8704\n",
            "Epoch: [232/1000] Step: [11800] Loss: 0.9647 Val Loss: 0.8684\n",
            "Epoch: [232/1000] Step: [11820] Loss: 0.9475 Val Loss: 0.8778\n",
            "Epoch: [233/1000] Step: [11840] Loss: 0.9747 Val Loss: 0.8717\n",
            "Epoch: [233/1000] Step: [11860] Loss: 0.9311 Val Loss: 0.8780\n",
            "Epoch: [233/1000] Step: [11880] Loss: 0.9491 Val Loss: 0.8679\n",
            "Epoch: [234/1000] Step: [11900] Loss: 0.9559 Val Loss: 0.8689\n",
            "Epoch: [234/1000] Step: [11920] Loss: 0.9399 Val Loss: 0.8737\n",
            "Epoch: [235/1000] Step: [11940] Loss: 0.8878 Val Loss: 0.8724\n",
            "Epoch: [235/1000] Step: [11960] Loss: 0.9084 Val Loss: 0.8709\n",
            "Epoch: [235/1000] Step: [11980] Loss: 0.8991 Val Loss: 0.8693\n",
            "Epoch: [236/1000] Step: [12000] Loss: 0.9313 Val Loss: 0.8721\n",
            "Epoch: [236/1000] Step: [12020] Loss: 0.9315 Val Loss: 0.8695\n",
            "Epoch: [237/1000] Step: [12040] Loss: 0.9120 Val Loss: 0.8722\n",
            "Epoch: [237/1000] Step: [12060] Loss: 0.9289 Val Loss: 0.8760\n",
            "Epoch: [237/1000] Step: [12080] Loss: 0.9483 Val Loss: 0.8692\n",
            "Epoch: [238/1000] Step: [12100] Loss: 0.9397 Val Loss: 0.8690\n",
            "Epoch: [238/1000] Step: [12120] Loss: 0.9291 Val Loss: 0.8715\n",
            "Epoch: [239/1000] Step: [12140] Loss: 0.9307 Val Loss: 0.8711\n",
            "Epoch: [239/1000] Step: [12160] Loss: 0.9964 Val Loss: 0.8712\n",
            "Epoch: [239/1000] Step: [12180] Loss: 0.9463 Val Loss: 0.8659\n",
            "Epoch: [240/1000] Step: [12200] Loss: 0.9599 Val Loss: 0.8708\n",
            "Epoch: [240/1000] Step: [12220] Loss: 0.9179 Val Loss: 0.8674\n",
            "Epoch: [240/1000] Step: [12240] Loss: 1.0761 Val Loss: 0.8709\n",
            "Epoch: [241/1000] Step: [12260] Loss: 0.9417 Val Loss: 0.8668\n",
            "Epoch: [241/1000] Step: [12280] Loss: 0.8946 Val Loss: 0.8650\n",
            "Epoch: [242/1000] Step: [12300] Loss: 0.9567 Val Loss: 0.8659\n",
            "Epoch: [242/1000] Step: [12320] Loss: 0.9355 Val Loss: 0.8777\n",
            "Epoch: [242/1000] Step: [12340] Loss: 0.9306 Val Loss: 0.8682\n",
            "Epoch: [243/1000] Step: [12360] Loss: 0.9526 Val Loss: 0.8649\n",
            "Epoch: [243/1000] Step: [12380] Loss: 0.9547 Val Loss: 0.8690\n",
            "Epoch: [244/1000] Step: [12400] Loss: 0.9088 Val Loss: 0.8700\n",
            "Epoch: [244/1000] Step: [12420] Loss: 0.9516 Val Loss: 0.8685\n",
            "Epoch: [244/1000] Step: [12440] Loss: 0.9008 Val Loss: 0.8639\n",
            "Epoch: [245/1000] Step: [12460] Loss: 0.9568 Val Loss: 0.8657\n",
            "Epoch: [245/1000] Step: [12480] Loss: 0.9295 Val Loss: 0.8669\n",
            "Epoch: [246/1000] Step: [12500] Loss: 0.9002 Val Loss: 0.8686\n",
            "Epoch: [246/1000] Step: [12520] Loss: 0.9113 Val Loss: 0.8656\n",
            "Epoch: [246/1000] Step: [12540] Loss: 0.9469 Val Loss: 0.8693\n",
            "Epoch: [247/1000] Step: [12560] Loss: 0.9268 Val Loss: 0.8710\n",
            "Epoch: [247/1000] Step: [12580] Loss: 0.9095 Val Loss: 0.8718\n",
            "Epoch: [248/1000] Step: [12600] Loss: 0.9216 Val Loss: 0.8669\n",
            "Epoch: [248/1000] Step: [12620] Loss: 0.9560 Val Loss: 0.8708\n",
            "Epoch: [248/1000] Step: [12640] Loss: 0.9490 Val Loss: 0.8712\n",
            "Epoch: [249/1000] Step: [12660] Loss: 1.0005 Val Loss: 0.8753\n",
            "Epoch: [249/1000] Step: [12680] Loss: 0.9360 Val Loss: 0.8762\n",
            "Epoch: [250/1000] Step: [12700] Loss: 0.9252 Val Loss: 0.8749\n",
            "Epoch: [250/1000] Step: [12720] Loss: 0.9773 Val Loss: 0.8700\n",
            "Epoch: [250/1000] Step: [12740] Loss: 0.9291 Val Loss: 0.8674\n",
            "Epoch: [251/1000] Step: [12760] Loss: 0.9653 Val Loss: 0.8682\n",
            "Epoch: [251/1000] Step: [12780] Loss: 0.9134 Val Loss: 0.8689\n",
            "Epoch: [251/1000] Step: [12800] Loss: 0.8877 Val Loss: 0.8679\n",
            "Epoch: [252/1000] Step: [12820] Loss: 0.9491 Val Loss: 0.8686\n",
            "Epoch: [252/1000] Step: [12840] Loss: 0.9521 Val Loss: 0.8744\n",
            "Epoch: [253/1000] Step: [12860] Loss: 0.9643 Val Loss: 0.8699\n",
            "Epoch: [253/1000] Step: [12880] Loss: 0.9252 Val Loss: 0.8727\n",
            "Epoch: [253/1000] Step: [12900] Loss: 0.9437 Val Loss: 0.8629\n",
            "Epoch: [254/1000] Step: [12920] Loss: 0.9447 Val Loss: 0.8740\n",
            "Epoch: [254/1000] Step: [12940] Loss: 0.9451 Val Loss: 0.8709\n",
            "Epoch: [255/1000] Step: [12960] Loss: 0.8914 Val Loss: 0.8718\n",
            "Epoch: [255/1000] Step: [12980] Loss: 0.8829 Val Loss: 0.8646\n",
            "Epoch: [255/1000] Step: [13000] Loss: 0.8861 Val Loss: 0.8651\n",
            "Epoch: [256/1000] Step: [13020] Loss: 0.9214 Val Loss: 0.8606\n",
            "Epoch: [256/1000] Step: [13040] Loss: 0.9136 Val Loss: 0.8692\n",
            "Epoch: [257/1000] Step: [13060] Loss: 0.9130 Val Loss: 0.8639\n",
            "Epoch: [257/1000] Step: [13080] Loss: 0.9149 Val Loss: 0.8670\n",
            "Epoch: [257/1000] Step: [13100] Loss: 0.9333 Val Loss: 0.8673\n",
            "Epoch: [258/1000] Step: [13120] Loss: 0.9361 Val Loss: 0.8727\n",
            "Epoch: [258/1000] Step: [13140] Loss: 0.9122 Val Loss: 0.8715\n",
            "Epoch: [259/1000] Step: [13160] Loss: 0.9284 Val Loss: 0.8733\n",
            "Epoch: [259/1000] Step: [13180] Loss: 0.9818 Val Loss: 0.8726\n",
            "Epoch: [259/1000] Step: [13200] Loss: 0.9587 Val Loss: 0.8740\n",
            "Epoch: [260/1000] Step: [13220] Loss: 0.9542 Val Loss: 0.8741\n",
            "Epoch: [260/1000] Step: [13240] Loss: 0.9269 Val Loss: 0.8699\n",
            "Epoch: [260/1000] Step: [13260] Loss: 1.0514 Val Loss: 0.8656\n",
            "Epoch: [261/1000] Step: [13280] Loss: 0.9573 Val Loss: 0.8641\n",
            "Epoch: [261/1000] Step: [13300] Loss: 0.8760 Val Loss: 0.8652\n",
            "Epoch: [262/1000] Step: [13320] Loss: 0.9203 Val Loss: 0.8642\n",
            "Epoch: [262/1000] Step: [13340] Loss: 0.9404 Val Loss: 0.8714\n",
            "Epoch: [262/1000] Step: [13360] Loss: 0.9268 Val Loss: 0.8672\n",
            "Epoch: [263/1000] Step: [13380] Loss: 0.9368 Val Loss: 0.8639\n",
            "Epoch: [263/1000] Step: [13400] Loss: 0.9325 Val Loss: 0.8684\n",
            "Epoch: [264/1000] Step: [13420] Loss: 0.9102 Val Loss: 0.8709\n",
            "Epoch: [264/1000] Step: [13440] Loss: 0.9308 Val Loss: 0.8643\n",
            "Epoch: [264/1000] Step: [13460] Loss: 0.8896 Val Loss: 0.8661\n",
            "Epoch: [265/1000] Step: [13480] Loss: 0.9737 Val Loss: 0.8743\n",
            "Epoch: [265/1000] Step: [13500] Loss: 0.9356 Val Loss: 0.8740\n",
            "Epoch: [266/1000] Step: [13520] Loss: 0.9130 Val Loss: 0.8724\n",
            "Epoch: [266/1000] Step: [13540] Loss: 0.9172 Val Loss: 0.8736\n",
            "Epoch: [266/1000] Step: [13560] Loss: 0.9520 Val Loss: 0.8692\n",
            "Epoch: [267/1000] Step: [13580] Loss: 0.9568 Val Loss: 0.8657\n",
            "Epoch: [267/1000] Step: [13600] Loss: 0.9235 Val Loss: 0.8631\n",
            "Epoch: [268/1000] Step: [13620] Loss: 0.9237 Val Loss: 0.8618\n",
            "Epoch: [268/1000] Step: [13640] Loss: 0.9312 Val Loss: 0.8673\n",
            "Epoch: [268/1000] Step: [13660] Loss: 0.9500 Val Loss: 0.8703\n",
            "Epoch: [269/1000] Step: [13680] Loss: 0.9572 Val Loss: 0.8714\n",
            "Epoch: [269/1000] Step: [13700] Loss: 0.9462 Val Loss: 0.8680\n",
            "Epoch: [270/1000] Step: [13720] Loss: 0.9329 Val Loss: 0.8678\n",
            "Epoch: [270/1000] Step: [13740] Loss: 1.0009 Val Loss: 0.8665\n",
            "Epoch: [270/1000] Step: [13760] Loss: 0.9351 Val Loss: 0.8710\n",
            "Epoch: [271/1000] Step: [13780] Loss: 0.9658 Val Loss: 0.8690\n",
            "Epoch: [271/1000] Step: [13800] Loss: 0.9271 Val Loss: 0.8698\n",
            "Epoch: [271/1000] Step: [13820] Loss: 0.8944 Val Loss: 0.8638\n",
            "Epoch: [272/1000] Step: [13840] Loss: 0.9689 Val Loss: 0.8643\n",
            "Epoch: [272/1000] Step: [13860] Loss: 0.9262 Val Loss: 0.8634\n",
            "Epoch: [273/1000] Step: [13880] Loss: 0.9429 Val Loss: 0.8618\n",
            "Epoch: [273/1000] Step: [13900] Loss: 0.9032 Val Loss: 0.8600\n",
            "Epoch: [273/1000] Step: [13920] Loss: 0.9413 Val Loss: 0.8646\n",
            "Epoch: [274/1000] Step: [13940] Loss: 0.9545 Val Loss: 0.8697\n",
            "Epoch: [274/1000] Step: [13960] Loss: 0.9457 Val Loss: 0.8593\n",
            "Epoch: [275/1000] Step: [13980] Loss: 0.8846 Val Loss: 0.8584\n",
            "Epoch: [275/1000] Step: [14000] Loss: 0.8986 Val Loss: 0.8570\n",
            "Epoch: [275/1000] Step: [14020] Loss: 0.8944 Val Loss: 0.8598\n",
            "Epoch: [276/1000] Step: [14040] Loss: 0.9180 Val Loss: 0.8676\n",
            "Epoch: [276/1000] Step: [14060] Loss: 0.9110 Val Loss: 0.8641\n",
            "Epoch: [277/1000] Step: [14080] Loss: 0.9282 Val Loss: 0.8591\n",
            "Epoch: [277/1000] Step: [14100] Loss: 0.9070 Val Loss: 0.8649\n",
            "Epoch: [277/1000] Step: [14120] Loss: 0.9194 Val Loss: 0.8609\n",
            "Epoch: [278/1000] Step: [14140] Loss: 0.9343 Val Loss: 0.8632\n",
            "Epoch: [278/1000] Step: [14160] Loss: 0.8967 Val Loss: 0.8620\n",
            "Epoch: [279/1000] Step: [14180] Loss: 0.9306 Val Loss: 0.8553\n",
            "Epoch: [279/1000] Step: [14200] Loss: 0.9634 Val Loss: 0.8616\n",
            "Epoch: [279/1000] Step: [14220] Loss: 0.9388 Val Loss: 0.8599\n",
            "Epoch: [280/1000] Step: [14240] Loss: 0.9688 Val Loss: 0.8570\n",
            "Epoch: [280/1000] Step: [14260] Loss: 0.9151 Val Loss: 0.8564\n",
            "Epoch: [280/1000] Step: [14280] Loss: 1.0638 Val Loss: 0.8538\n",
            "Epoch: [281/1000] Step: [14300] Loss: 0.9456 Val Loss: 0.8597\n",
            "Epoch: [281/1000] Step: [14320] Loss: 0.8677 Val Loss: 0.8593\n",
            "Epoch: [282/1000] Step: [14340] Loss: 0.9316 Val Loss: 0.8562\n",
            "Epoch: [282/1000] Step: [14360] Loss: 0.9155 Val Loss: 0.8624\n",
            "Epoch: [282/1000] Step: [14380] Loss: 0.9140 Val Loss: 0.8567\n",
            "Epoch: [283/1000] Step: [14400] Loss: 0.9559 Val Loss: 0.8705\n",
            "Epoch: [283/1000] Step: [14420] Loss: 0.9439 Val Loss: 0.8672\n",
            "Epoch: [284/1000] Step: [14440] Loss: 0.8962 Val Loss: 0.8634\n",
            "Epoch: [284/1000] Step: [14460] Loss: 0.9270 Val Loss: 0.8688\n",
            "Epoch: [284/1000] Step: [14480] Loss: 0.8828 Val Loss: 0.8648\n",
            "Epoch: [285/1000] Step: [14500] Loss: 0.9514 Val Loss: 0.8631\n",
            "Epoch: [285/1000] Step: [14520] Loss: 0.9305 Val Loss: 0.8631\n",
            "Epoch: [286/1000] Step: [14540] Loss: 0.8859 Val Loss: 0.8683\n",
            "Epoch: [286/1000] Step: [14560] Loss: 0.9439 Val Loss: 0.8774\n",
            "Epoch: [286/1000] Step: [14580] Loss: 0.9644 Val Loss: 0.8717\n",
            "Epoch: [287/1000] Step: [14600] Loss: 0.9422 Val Loss: 0.8656\n",
            "Epoch: [287/1000] Step: [14620] Loss: 0.9298 Val Loss: 0.8685\n",
            "Epoch: [288/1000] Step: [14640] Loss: 0.9152 Val Loss: 0.8652\n",
            "Epoch: [288/1000] Step: [14660] Loss: 0.9478 Val Loss: 0.8665\n",
            "Epoch: [288/1000] Step: [14680] Loss: 0.9716 Val Loss: 0.8732\n",
            "Epoch: [289/1000] Step: [14700] Loss: 0.9894 Val Loss: 0.8670\n",
            "Epoch: [289/1000] Step: [14720] Loss: 0.9372 Val Loss: 0.8686\n",
            "Epoch: [290/1000] Step: [14740] Loss: 0.9288 Val Loss: 0.8611\n",
            "Epoch: [290/1000] Step: [14760] Loss: 1.0007 Val Loss: 0.8627\n",
            "Epoch: [290/1000] Step: [14780] Loss: 0.9528 Val Loss: 0.8698\n",
            "Epoch: [291/1000] Step: [14800] Loss: 0.9661 Val Loss: 0.8652\n",
            "Epoch: [291/1000] Step: [14820] Loss: 0.9352 Val Loss: 0.8782\n",
            "Epoch: [291/1000] Step: [14840] Loss: 0.8761 Val Loss: 0.8650\n",
            "Epoch: [292/1000] Step: [14860] Loss: 0.9726 Val Loss: 0.8724\n",
            "Epoch: [292/1000] Step: [14880] Loss: 0.9470 Val Loss: 0.8693\n",
            "Epoch: [293/1000] Step: [14900] Loss: 0.9503 Val Loss: 0.8687\n",
            "Epoch: [293/1000] Step: [14920] Loss: 0.9095 Val Loss: 0.8640\n",
            "Epoch: [293/1000] Step: [14940] Loss: 0.9251 Val Loss: 0.8580\n",
            "Epoch: [294/1000] Step: [14960] Loss: 0.9543 Val Loss: 0.8651\n",
            "Epoch: [294/1000] Step: [14980] Loss: 0.9620 Val Loss: 0.8607\n",
            "Epoch: [295/1000] Step: [15000] Loss: 0.8839 Val Loss: 0.8631\n",
            "Epoch: [295/1000] Step: [15020] Loss: 0.9008 Val Loss: 0.8719\n",
            "Epoch: [295/1000] Step: [15040] Loss: 0.9051 Val Loss: 0.8649\n",
            "Epoch: [296/1000] Step: [15060] Loss: 0.9102 Val Loss: 0.8648\n",
            "Epoch: [296/1000] Step: [15080] Loss: 0.9262 Val Loss: 0.8688\n",
            "Epoch: [297/1000] Step: [15100] Loss: 0.9229 Val Loss: 0.8660\n",
            "Epoch: [297/1000] Step: [15120] Loss: 0.9027 Val Loss: 0.8618\n",
            "Epoch: [297/1000] Step: [15140] Loss: 0.9247 Val Loss: 0.8618\n",
            "Epoch: [298/1000] Step: [15160] Loss: 0.9468 Val Loss: 0.8622\n",
            "Epoch: [298/1000] Step: [15180] Loss: 0.8869 Val Loss: 0.8664\n",
            "Epoch: [299/1000] Step: [15200] Loss: 0.9226 Val Loss: 0.8620\n",
            "Epoch: [299/1000] Step: [15220] Loss: 0.9766 Val Loss: 0.8635\n",
            "Epoch: [299/1000] Step: [15240] Loss: 0.9415 Val Loss: 0.8641\n",
            "Epoch: [300/1000] Step: [15260] Loss: 0.9505 Val Loss: 0.8576\n",
            "Epoch: [300/1000] Step: [15280] Loss: 0.9224 Val Loss: 0.8607\n",
            "Epoch: [300/1000] Step: [15300] Loss: 1.0480 Val Loss: 0.8487\n",
            "Epoch: [301/1000] Step: [15320] Loss: 0.9161 Val Loss: 0.8595\n",
            "Epoch: [301/1000] Step: [15340] Loss: 0.8592 Val Loss: 0.8628\n",
            "Epoch: [302/1000] Step: [15360] Loss: 0.9293 Val Loss: 0.8673\n",
            "Epoch: [302/1000] Step: [15380] Loss: 0.9339 Val Loss: 0.8649\n",
            "Epoch: [302/1000] Step: [15400] Loss: 0.9219 Val Loss: 0.8602\n",
            "Epoch: [303/1000] Step: [15420] Loss: 0.9676 Val Loss: 0.8778\n",
            "Epoch: [303/1000] Step: [15440] Loss: 0.9609 Val Loss: 0.8669\n",
            "Epoch: [304/1000] Step: [15460] Loss: 0.9037 Val Loss: 0.8719\n",
            "Epoch: [304/1000] Step: [15480] Loss: 0.9205 Val Loss: 0.8713\n",
            "Epoch: [304/1000] Step: [15500] Loss: 0.8978 Val Loss: 0.8636\n",
            "Epoch: [305/1000] Step: [15520] Loss: 0.9540 Val Loss: 0.8837\n",
            "Epoch: [305/1000] Step: [15540] Loss: 0.9481 Val Loss: 0.8605\n",
            "Epoch: [306/1000] Step: [15560] Loss: 0.9081 Val Loss: 0.8676\n",
            "Epoch: [306/1000] Step: [15580] Loss: 0.9149 Val Loss: 0.8622\n",
            "Epoch: [306/1000] Step: [15600] Loss: 0.9316 Val Loss: 0.8581\n",
            "Epoch: [307/1000] Step: [15620] Loss: 0.9379 Val Loss: 0.8642\n",
            "Epoch: [307/1000] Step: [15640] Loss: 0.9001 Val Loss: 0.8614\n",
            "Epoch: [308/1000] Step: [15660] Loss: 0.9170 Val Loss: 0.8619\n",
            "Epoch: [308/1000] Step: [15680] Loss: 0.9642 Val Loss: 0.8651\n",
            "Epoch: [308/1000] Step: [15700] Loss: 0.9598 Val Loss: 0.8632\n",
            "Epoch: [309/1000] Step: [15720] Loss: 0.9533 Val Loss: 0.8617\n",
            "Epoch: [309/1000] Step: [15740] Loss: 0.9459 Val Loss: 0.8640\n",
            "Epoch: [310/1000] Step: [15760] Loss: 0.9057 Val Loss: 0.8608\n",
            "Epoch: [310/1000] Step: [15780] Loss: 0.9882 Val Loss: 0.8617\n",
            "Epoch: [310/1000] Step: [15800] Loss: 0.9340 Val Loss: 0.8623\n",
            "Epoch: [311/1000] Step: [15820] Loss: 0.9568 Val Loss: 0.8592\n",
            "Epoch: [311/1000] Step: [15840] Loss: 0.9328 Val Loss: 0.8666\n",
            "Epoch: [311/1000] Step: [15860] Loss: 0.8788 Val Loss: 0.8588\n",
            "Epoch: [312/1000] Step: [15880] Loss: 0.9467 Val Loss: 0.8575\n",
            "Epoch: [312/1000] Step: [15900] Loss: 0.9328 Val Loss: 0.8597\n",
            "Epoch: [313/1000] Step: [15920] Loss: 0.9428 Val Loss: 0.8606\n",
            "Epoch: [313/1000] Step: [15940] Loss: 0.9127 Val Loss: 0.8618\n",
            "Epoch: [313/1000] Step: [15960] Loss: 0.9066 Val Loss: 0.8590\n",
            "Epoch: [314/1000] Step: [15980] Loss: 0.9557 Val Loss: 0.8588\n",
            "Epoch: [314/1000] Step: [16000] Loss: 0.9379 Val Loss: 0.8571\n",
            "Epoch: [315/1000] Step: [16020] Loss: 0.8732 Val Loss: 0.8500\n",
            "Epoch: [315/1000] Step: [16040] Loss: 0.8735 Val Loss: 0.8507\n",
            "Epoch: [315/1000] Step: [16060] Loss: 0.9019 Val Loss: 0.8521\n",
            "Epoch: [316/1000] Step: [16080] Loss: 0.9143 Val Loss: 0.8559\n",
            "Epoch: [316/1000] Step: [16100] Loss: 0.9118 Val Loss: 0.8567\n",
            "Epoch: [317/1000] Step: [16120] Loss: 0.9098 Val Loss: 0.8519\n",
            "Epoch: [317/1000] Step: [16140] Loss: 0.9127 Val Loss: 0.8551\n",
            "Epoch: [317/1000] Step: [16160] Loss: 0.9302 Val Loss: 0.8554\n",
            "Epoch: [318/1000] Step: [16180] Loss: 0.9541 Val Loss: 0.8588\n",
            "Epoch: [318/1000] Step: [16200] Loss: 0.9202 Val Loss: 0.8518\n",
            "Epoch: [319/1000] Step: [16220] Loss: 0.9249 Val Loss: 0.8567\n",
            "Epoch: [319/1000] Step: [16240] Loss: 0.9727 Val Loss: 0.8498\n",
            "Epoch: [319/1000] Step: [16260] Loss: 0.9289 Val Loss: 0.8566\n",
            "Epoch: [320/1000] Step: [16280] Loss: 0.9517 Val Loss: 0.8574\n",
            "Epoch: [320/1000] Step: [16300] Loss: 0.9015 Val Loss: 0.8634\n",
            "Epoch: [320/1000] Step: [16320] Loss: 1.0474 Val Loss: 0.8571\n",
            "Epoch: [321/1000] Step: [16340] Loss: 0.9504 Val Loss: 0.8624\n",
            "Epoch: [321/1000] Step: [16360] Loss: 0.8633 Val Loss: 0.8564\n",
            "Epoch: [322/1000] Step: [16380] Loss: 0.9270 Val Loss: 0.8568\n",
            "Epoch: [322/1000] Step: [16400] Loss: 0.9278 Val Loss: 0.8570\n",
            "Epoch: [322/1000] Step: [16420] Loss: 0.9302 Val Loss: 0.8620\n",
            "Epoch: [323/1000] Step: [16440] Loss: 0.9569 Val Loss: 0.8607\n",
            "Epoch: [323/1000] Step: [16460] Loss: 0.9245 Val Loss: 0.8534\n",
            "Epoch: [324/1000] Step: [16480] Loss: 0.8979 Val Loss: 0.8539\n",
            "Epoch: [324/1000] Step: [16500] Loss: 0.9215 Val Loss: 0.8580\n",
            "Epoch: [324/1000] Step: [16520] Loss: 0.8689 Val Loss: 0.8529\n",
            "Epoch: [325/1000] Step: [16540] Loss: 0.9367 Val Loss: 0.8515\n",
            "Epoch: [325/1000] Step: [16560] Loss: 0.9235 Val Loss: 0.8597\n",
            "Epoch: [326/1000] Step: [16580] Loss: 0.8874 Val Loss: 0.8551\n",
            "Epoch: [326/1000] Step: [16600] Loss: 0.9076 Val Loss: 0.8609\n",
            "Epoch: [326/1000] Step: [16620] Loss: 0.9206 Val Loss: 0.8557\n",
            "Epoch: [327/1000] Step: [16640] Loss: 0.9355 Val Loss: 0.8608\n",
            "Epoch: [327/1000] Step: [16660] Loss: 0.9213 Val Loss: 0.8557\n",
            "Epoch: [328/1000] Step: [16680] Loss: 0.8987 Val Loss: 0.8575\n",
            "Epoch: [328/1000] Step: [16700] Loss: 0.9478 Val Loss: 0.8530\n",
            "Epoch: [328/1000] Step: [16720] Loss: 0.9407 Val Loss: 0.8497\n",
            "Epoch: [329/1000] Step: [16740] Loss: 0.9556 Val Loss: 0.8590\n",
            "Epoch: [329/1000] Step: [16760] Loss: 0.9429 Val Loss: 0.8613\n",
            "Epoch: [330/1000] Step: [16780] Loss: 0.9075 Val Loss: 0.8553\n",
            "Epoch: [330/1000] Step: [16800] Loss: 0.9834 Val Loss: 0.8523\n",
            "Epoch: [330/1000] Step: [16820] Loss: 0.9167 Val Loss: 0.8565\n",
            "Epoch: [331/1000] Step: [16840] Loss: 0.9520 Val Loss: 0.8588\n",
            "Epoch: [331/1000] Step: [16860] Loss: 0.9075 Val Loss: 0.8570\n",
            "Epoch: [331/1000] Step: [16880] Loss: 0.8644 Val Loss: 0.8561\n",
            "Epoch: [332/1000] Step: [16900] Loss: 0.9548 Val Loss: 0.8590\n",
            "Epoch: [332/1000] Step: [16920] Loss: 0.9126 Val Loss: 0.8596\n",
            "Epoch: [333/1000] Step: [16940] Loss: 0.9314 Val Loss: 0.8512\n",
            "Epoch: [333/1000] Step: [16960] Loss: 0.9109 Val Loss: 0.8609\n",
            "Epoch: [333/1000] Step: [16980] Loss: 0.9045 Val Loss: 0.8570\n",
            "Epoch: [334/1000] Step: [17000] Loss: 0.9455 Val Loss: 0.8574\n",
            "Epoch: [334/1000] Step: [17020] Loss: 0.9293 Val Loss: 0.8614\n",
            "Epoch: [335/1000] Step: [17040] Loss: 0.8909 Val Loss: 0.8587\n",
            "Epoch: [335/1000] Step: [17060] Loss: 0.8948 Val Loss: 0.8591\n",
            "Epoch: [335/1000] Step: [17080] Loss: 0.9029 Val Loss: 0.8640\n",
            "Epoch: [336/1000] Step: [17100] Loss: 0.9133 Val Loss: 0.8571\n",
            "Epoch: [336/1000] Step: [17120] Loss: 0.8976 Val Loss: 0.8741\n",
            "Epoch: [337/1000] Step: [17140] Loss: 0.9255 Val Loss: 0.8565\n",
            "Epoch: [337/1000] Step: [17160] Loss: 0.9256 Val Loss: 0.8646\n",
            "Epoch: [337/1000] Step: [17180] Loss: 0.9322 Val Loss: 0.8602\n",
            "Epoch: [338/1000] Step: [17200] Loss: 0.9358 Val Loss: 0.8669\n",
            "Epoch: [338/1000] Step: [17220] Loss: 0.9007 Val Loss: 0.8600\n",
            "Epoch: [339/1000] Step: [17240] Loss: 0.9286 Val Loss: 0.8630\n",
            "Epoch: [339/1000] Step: [17260] Loss: 0.9770 Val Loss: 0.8563\n",
            "Epoch: [339/1000] Step: [17280] Loss: 0.9422 Val Loss: 0.8577\n",
            "Epoch: [340/1000] Step: [17300] Loss: 0.9575 Val Loss: 0.8588\n",
            "Epoch: [340/1000] Step: [17320] Loss: 0.9224 Val Loss: 0.8574\n",
            "Epoch: [340/1000] Step: [17340] Loss: 1.0448 Val Loss: 0.8564\n",
            "Epoch: [341/1000] Step: [17360] Loss: 0.9277 Val Loss: 0.8560\n",
            "Epoch: [341/1000] Step: [17380] Loss: 0.8785 Val Loss: 0.8536\n",
            "Epoch: [342/1000] Step: [17400] Loss: 0.9098 Val Loss: 0.8578\n",
            "Epoch: [342/1000] Step: [17420] Loss: 0.9214 Val Loss: 0.8590\n",
            "Epoch: [342/1000] Step: [17440] Loss: 0.9053 Val Loss: 0.8518\n",
            "Epoch: [343/1000] Step: [17460] Loss: 0.9223 Val Loss: 0.8613\n",
            "Epoch: [343/1000] Step: [17480] Loss: 0.9366 Val Loss: 0.8524\n",
            "Epoch: [344/1000] Step: [17500] Loss: 0.9041 Val Loss: 0.8594\n",
            "Epoch: [344/1000] Step: [17520] Loss: 0.9070 Val Loss: 0.8479\n",
            "Epoch: [344/1000] Step: [17540] Loss: 0.8641 Val Loss: 0.8594\n",
            "Epoch: [345/1000] Step: [17560] Loss: 0.9410 Val Loss: 0.8522\n",
            "Epoch: [345/1000] Step: [17580] Loss: 0.9151 Val Loss: 0.8491\n",
            "Epoch: [346/1000] Step: [17600] Loss: 0.8706 Val Loss: 0.8511\n",
            "Epoch: [346/1000] Step: [17620] Loss: 0.9161 Val Loss: 0.8561\n",
            "Epoch: [346/1000] Step: [17640] Loss: 0.9299 Val Loss: 0.8565\n",
            "Epoch: [347/1000] Step: [17660] Loss: 0.9206 Val Loss: 0.8547\n",
            "Epoch: [347/1000] Step: [17680] Loss: 0.9153 Val Loss: 0.8615\n",
            "Epoch: [348/1000] Step: [17700] Loss: 0.9190 Val Loss: 0.8578\n",
            "Epoch: [348/1000] Step: [17720] Loss: 0.9212 Val Loss: 0.8558\n",
            "Epoch: [348/1000] Step: [17740] Loss: 0.9613 Val Loss: 0.8571\n",
            "Epoch: [349/1000] Step: [17760] Loss: 0.9599 Val Loss: 0.8506\n",
            "Epoch: [349/1000] Step: [17780] Loss: 0.9112 Val Loss: 0.8542\n",
            "Epoch: [350/1000] Step: [17800] Loss: 0.9054 Val Loss: 0.8541\n",
            "Epoch: [350/1000] Step: [17820] Loss: 0.9750 Val Loss: 0.8589\n",
            "Epoch: [350/1000] Step: [17840] Loss: 0.9297 Val Loss: 0.8550\n",
            "Epoch: [351/1000] Step: [17860] Loss: 0.9472 Val Loss: 0.8556\n",
            "Epoch: [351/1000] Step: [17880] Loss: 0.9348 Val Loss: 0.8561\n",
            "Epoch: [351/1000] Step: [17900] Loss: 0.8709 Val Loss: 0.8529\n",
            "Epoch: [352/1000] Step: [17920] Loss: 0.9371 Val Loss: 0.8497\n",
            "Epoch: [352/1000] Step: [17940] Loss: 0.9374 Val Loss: 0.8582\n",
            "Epoch: [353/1000] Step: [17960] Loss: 0.9634 Val Loss: 0.8643\n",
            "Epoch: [353/1000] Step: [17980] Loss: 0.9368 Val Loss: 0.8665\n",
            "Epoch: [353/1000] Step: [18000] Loss: 0.9207 Val Loss: 0.8558\n",
            "Epoch: [354/1000] Step: [18020] Loss: 0.9530 Val Loss: 0.8597\n",
            "Epoch: [354/1000] Step: [18040] Loss: 0.9367 Val Loss: 0.8550\n",
            "Epoch: [355/1000] Step: [18060] Loss: 0.8693 Val Loss: 0.8579\n",
            "Epoch: [355/1000] Step: [18080] Loss: 0.9028 Val Loss: 0.8595\n",
            "Epoch: [355/1000] Step: [18100] Loss: 0.8896 Val Loss: 0.8550\n",
            "Epoch: [356/1000] Step: [18120] Loss: 0.9107 Val Loss: 0.8543\n",
            "Epoch: [356/1000] Step: [18140] Loss: 0.9016 Val Loss: 0.8500\n",
            "Epoch: [357/1000] Step: [18160] Loss: 0.9195 Val Loss: 0.8556\n",
            "Epoch: [357/1000] Step: [18180] Loss: 0.9144 Val Loss: 0.8606\n",
            "Epoch: [357/1000] Step: [18200] Loss: 0.9501 Val Loss: 0.8699\n",
            "Epoch: [358/1000] Step: [18220] Loss: 0.9464 Val Loss: 0.8678\n",
            "Epoch: [358/1000] Step: [18240] Loss: 0.9144 Val Loss: 0.8677\n",
            "Epoch: [359/1000] Step: [18260] Loss: 0.9348 Val Loss: 0.8633\n",
            "Epoch: [359/1000] Step: [18280] Loss: 0.9946 Val Loss: 0.8704\n",
            "Epoch: [359/1000] Step: [18300] Loss: 0.9452 Val Loss: 0.8666\n",
            "Epoch: [360/1000] Step: [18320] Loss: 0.9594 Val Loss: 0.8674\n",
            "Epoch: [360/1000] Step: [18340] Loss: 0.9168 Val Loss: 0.8650\n",
            "Epoch: [360/1000] Step: [18360] Loss: 1.0639 Val Loss: 0.8629\n",
            "Epoch: [361/1000] Step: [18380] Loss: 0.9585 Val Loss: 0.8693\n",
            "Epoch: [361/1000] Step: [18400] Loss: 0.8738 Val Loss: 0.8653\n",
            "Epoch: [362/1000] Step: [18420] Loss: 0.9425 Val Loss: 0.8675\n",
            "Epoch: [362/1000] Step: [18440] Loss: 0.9307 Val Loss: 0.8646\n",
            "Epoch: [362/1000] Step: [18460] Loss: 0.9464 Val Loss: 0.8705\n",
            "Epoch: [363/1000] Step: [18480] Loss: 0.9594 Val Loss: 0.8679\n",
            "Epoch: [363/1000] Step: [18500] Loss: 0.9467 Val Loss: 0.8681\n",
            "Epoch: [364/1000] Step: [18520] Loss: 0.9022 Val Loss: 0.8597\n",
            "Epoch: [364/1000] Step: [18540] Loss: 0.9272 Val Loss: 0.8664\n",
            "Epoch: [364/1000] Step: [18560] Loss: 0.9032 Val Loss: 0.8732\n",
            "Epoch: [365/1000] Step: [18580] Loss: 0.9708 Val Loss: 0.8707\n",
            "Epoch: [365/1000] Step: [18600] Loss: 0.9307 Val Loss: 0.8706\n",
            "Epoch: [366/1000] Step: [18620] Loss: 0.8972 Val Loss: 0.8617\n",
            "Epoch: [366/1000] Step: [18640] Loss: 0.9356 Val Loss: 0.8641\n",
            "Epoch: [366/1000] Step: [18660] Loss: 0.9391 Val Loss: 0.8591\n",
            "Epoch: [367/1000] Step: [18680] Loss: 0.9356 Val Loss: 0.8596\n",
            "Epoch: [367/1000] Step: [18700] Loss: 0.9191 Val Loss: 0.8655\n",
            "Epoch: [368/1000] Step: [18720] Loss: 0.9257 Val Loss: 0.8595\n",
            "Epoch: [368/1000] Step: [18740] Loss: 0.9638 Val Loss: 0.8677\n",
            "Epoch: [368/1000] Step: [18760] Loss: 0.9592 Val Loss: 0.8650\n",
            "Epoch: [369/1000] Step: [18780] Loss: 0.9842 Val Loss: 0.8616\n",
            "Epoch: [369/1000] Step: [18800] Loss: 0.9420 Val Loss: 0.8633\n",
            "Epoch: [370/1000] Step: [18820] Loss: 0.9116 Val Loss: 0.8651\n",
            "Epoch: [370/1000] Step: [18840] Loss: 0.9859 Val Loss: 0.8590\n",
            "Epoch: [370/1000] Step: [18860] Loss: 0.9488 Val Loss: 0.8620\n",
            "Epoch: [371/1000] Step: [18880] Loss: 0.9623 Val Loss: 0.8577\n",
            "Epoch: [371/1000] Step: [18900] Loss: 0.9237 Val Loss: 0.8573\n",
            "Epoch: [371/1000] Step: [18920] Loss: 0.8720 Val Loss: 0.8552\n",
            "Epoch: [372/1000] Step: [18940] Loss: 0.9613 Val Loss: 0.8547\n",
            "Epoch: [372/1000] Step: [18960] Loss: 0.9172 Val Loss: 0.8533\n",
            "Epoch: [373/1000] Step: [18980] Loss: 0.9282 Val Loss: 0.8529\n",
            "Epoch: [373/1000] Step: [19000] Loss: 0.9085 Val Loss: 0.8522\n",
            "Epoch: [373/1000] Step: [19020] Loss: 0.9202 Val Loss: 0.8551\n",
            "Epoch: [374/1000] Step: [19040] Loss: 0.9307 Val Loss: 0.8583\n",
            "Epoch: [374/1000] Step: [19060] Loss: 0.9435 Val Loss: 0.8529\n",
            "Epoch: [375/1000] Step: [19080] Loss: 0.8797 Val Loss: 0.8546\n",
            "Epoch: [375/1000] Step: [19100] Loss: 0.8920 Val Loss: 0.8569\n",
            "Epoch: [375/1000] Step: [19120] Loss: 0.9083 Val Loss: 0.8524\n",
            "Epoch: [376/1000] Step: [19140] Loss: 0.9182 Val Loss: 0.8546\n",
            "Epoch: [376/1000] Step: [19160] Loss: 0.9237 Val Loss: 0.8570\n",
            "Epoch: [377/1000] Step: [19180] Loss: 0.9158 Val Loss: 0.8610\n",
            "Epoch: [377/1000] Step: [19200] Loss: 0.9220 Val Loss: 0.8613\n",
            "Epoch: [377/1000] Step: [19220] Loss: 0.9416 Val Loss: 0.8618\n",
            "Epoch: [378/1000] Step: [19240] Loss: 0.9455 Val Loss: 0.8523\n",
            "Epoch: [378/1000] Step: [19260] Loss: 0.9138 Val Loss: 0.8625\n",
            "Epoch: [379/1000] Step: [19280] Loss: 0.9327 Val Loss: 0.8642\n",
            "Epoch: [379/1000] Step: [19300] Loss: 0.9904 Val Loss: 0.8566\n",
            "Epoch: [379/1000] Step: [19320] Loss: 0.9318 Val Loss: 0.8517\n",
            "Epoch: [380/1000] Step: [19340] Loss: 0.9634 Val Loss: 0.8559\n",
            "Epoch: [380/1000] Step: [19360] Loss: 0.9104 Val Loss: 0.8544\n",
            "Epoch: [380/1000] Step: [19380] Loss: 1.0405 Val Loss: 0.8521\n",
            "Epoch: [381/1000] Step: [19400] Loss: 0.9417 Val Loss: 0.8564\n",
            "Epoch: [381/1000] Step: [19420] Loss: 0.8849 Val Loss: 0.8591\n",
            "Epoch: [382/1000] Step: [19440] Loss: 0.9097 Val Loss: 0.8531\n",
            "Epoch: [382/1000] Step: [19460] Loss: 0.9286 Val Loss: 0.8563\n",
            "Epoch: [382/1000] Step: [19480] Loss: 0.9273 Val Loss: 0.8568\n",
            "Epoch: [383/1000] Step: [19500] Loss: 0.9488 Val Loss: 0.8524\n",
            "Epoch: [383/1000] Step: [19520] Loss: 0.9487 Val Loss: 0.8544\n",
            "Epoch: [384/1000] Step: [19540] Loss: 0.9223 Val Loss: 0.8675\n",
            "Epoch: [384/1000] Step: [19560] Loss: 0.9211 Val Loss: 0.8687\n",
            "Epoch: [384/1000] Step: [19580] Loss: 0.8923 Val Loss: 0.8685\n",
            "Epoch: [385/1000] Step: [19600] Loss: 0.9664 Val Loss: 0.8668\n",
            "Epoch: [385/1000] Step: [19620] Loss: 0.9720 Val Loss: 0.8827\n",
            "Epoch: [386/1000] Step: [19640] Loss: 0.9259 Val Loss: 0.8719\n",
            "Epoch: [386/1000] Step: [19660] Loss: 0.9397 Val Loss: 0.8636\n",
            "Epoch: [386/1000] Step: [19680] Loss: 0.9715 Val Loss: 0.8658\n",
            "Epoch: [387/1000] Step: [19700] Loss: 0.9374 Val Loss: 0.8693\n",
            "Epoch: [387/1000] Step: [19720] Loss: 0.9187 Val Loss: 0.8610\n",
            "Epoch: [388/1000] Step: [19740] Loss: 0.9182 Val Loss: 0.8637\n",
            "Epoch: [388/1000] Step: [19760] Loss: 0.9450 Val Loss: 0.8578\n",
            "Epoch: [388/1000] Step: [19780] Loss: 0.9692 Val Loss: 0.8553\n",
            "Epoch: [389/1000] Step: [19800] Loss: 0.9865 Val Loss: 0.8613\n",
            "Epoch: [389/1000] Step: [19820] Loss: 0.9435 Val Loss: 0.8580\n",
            "Epoch: [390/1000] Step: [19840] Loss: 0.9137 Val Loss: 0.8631\n",
            "Epoch: [390/1000] Step: [19860] Loss: 0.9806 Val Loss: 0.8630\n",
            "Epoch: [390/1000] Step: [19880] Loss: 0.9392 Val Loss: 0.8547\n",
            "Epoch: [391/1000] Step: [19900] Loss: 0.9436 Val Loss: 0.8539\n",
            "Epoch: [391/1000] Step: [19920] Loss: 0.9304 Val Loss: 0.8552\n",
            "Epoch: [391/1000] Step: [19940] Loss: 0.8748 Val Loss: 0.8516\n",
            "Epoch: [392/1000] Step: [19960] Loss: 0.9438 Val Loss: 0.8490\n",
            "Epoch: [392/1000] Step: [19980] Loss: 0.9145 Val Loss: 0.8582\n",
            "Epoch: [393/1000] Step: [20000] Loss: 0.9551 Val Loss: 0.8547\n",
            "Epoch: [393/1000] Step: [20020] Loss: 0.9004 Val Loss: 0.8499\n",
            "Epoch: [393/1000] Step: [20040] Loss: 0.9176 Val Loss: 0.8480\n",
            "Epoch: [394/1000] Step: [20060] Loss: 0.9404 Val Loss: 0.8523\n",
            "Epoch: [394/1000] Step: [20080] Loss: 0.9036 Val Loss: 0.8498\n",
            "Epoch: [395/1000] Step: [20100] Loss: 0.8718 Val Loss: 0.8503\n",
            "Epoch: [395/1000] Step: [20120] Loss: 0.8748 Val Loss: 0.8575\n",
            "Epoch: [395/1000] Step: [20140] Loss: 0.8880 Val Loss: 0.8560\n",
            "Epoch: [396/1000] Step: [20160] Loss: 0.9130 Val Loss: 0.8576\n",
            "Epoch: [396/1000] Step: [20180] Loss: 0.9133 Val Loss: 0.8553\n",
            "Epoch: [397/1000] Step: [20200] Loss: 0.9022 Val Loss: 0.8595\n",
            "Epoch: [397/1000] Step: [20220] Loss: 0.8946 Val Loss: 0.8551\n",
            "Epoch: [397/1000] Step: [20240] Loss: 0.9055 Val Loss: 0.8514\n",
            "Epoch: [398/1000] Step: [20260] Loss: 0.9236 Val Loss: 0.8643\n",
            "Epoch: [398/1000] Step: [20280] Loss: 0.9125 Val Loss: 0.8536\n",
            "Epoch: [399/1000] Step: [20300] Loss: 0.9161 Val Loss: 0.8550\n",
            "Epoch: [399/1000] Step: [20320] Loss: 0.9618 Val Loss: 0.8550\n",
            "Epoch: [399/1000] Step: [20340] Loss: 0.9283 Val Loss: 0.8601\n",
            "Epoch: [400/1000] Step: [20360] Loss: 0.9338 Val Loss: 0.8548\n",
            "Epoch: [400/1000] Step: [20380] Loss: 0.9231 Val Loss: 0.8629\n",
            "Epoch: [400/1000] Step: [20400] Loss: 1.0624 Val Loss: 0.8600\n",
            "Epoch: [401/1000] Step: [20420] Loss: 0.9280 Val Loss: 0.8555\n",
            "Epoch: [401/1000] Step: [20440] Loss: 0.8808 Val Loss: 0.8504\n",
            "Epoch: [402/1000] Step: [20460] Loss: 0.9127 Val Loss: 0.8572\n",
            "Epoch: [402/1000] Step: [20480] Loss: 0.9363 Val Loss: 0.8628\n",
            "Epoch: [402/1000] Step: [20500] Loss: 0.9175 Val Loss: 0.8500\n",
            "Epoch: [403/1000] Step: [20520] Loss: 0.9349 Val Loss: 0.8520\n",
            "Epoch: [403/1000] Step: [20540] Loss: 0.9447 Val Loss: 0.8519\n",
            "Epoch: [404/1000] Step: [20560] Loss: 0.8989 Val Loss: 0.8535\n",
            "Epoch: [404/1000] Step: [20580] Loss: 0.9093 Val Loss: 0.8582\n",
            "Epoch: [404/1000] Step: [20600] Loss: 0.8851 Val Loss: 0.8533\n",
            "Epoch: [405/1000] Step: [20620] Loss: 0.9272 Val Loss: 0.8554\n",
            "Epoch: [405/1000] Step: [20640] Loss: 0.9186 Val Loss: 0.8540\n",
            "Epoch: [406/1000] Step: [20660] Loss: 0.9025 Val Loss: 0.8562\n",
            "Epoch: [406/1000] Step: [20680] Loss: 0.9104 Val Loss: 0.8553\n",
            "Epoch: [406/1000] Step: [20700] Loss: 0.9323 Val Loss: 0.8561\n",
            "Epoch: [407/1000] Step: [20720] Loss: 0.9334 Val Loss: 0.8541\n",
            "Epoch: [407/1000] Step: [20740] Loss: 0.8919 Val Loss: 0.8506\n",
            "Epoch: [408/1000] Step: [20760] Loss: 0.9076 Val Loss: 0.8533\n",
            "Epoch: [408/1000] Step: [20780] Loss: 0.9419 Val Loss: 0.8577\n",
            "Epoch: [408/1000] Step: [20800] Loss: 0.9603 Val Loss: 0.8519\n",
            "Epoch: [409/1000] Step: [20820] Loss: 0.9756 Val Loss: 0.8564\n",
            "Epoch: [409/1000] Step: [20840] Loss: 0.9203 Val Loss: 0.8549\n",
            "Epoch: [410/1000] Step: [20860] Loss: 0.9070 Val Loss: 0.8506\n",
            "Epoch: [410/1000] Step: [20880] Loss: 0.9895 Val Loss: 0.8577\n",
            "Epoch: [410/1000] Step: [20900] Loss: 0.9447 Val Loss: 0.8591\n",
            "Epoch: [411/1000] Step: [20920] Loss: 0.9760 Val Loss: 0.8650\n",
            "Epoch: [411/1000] Step: [20940] Loss: 0.9321 Val Loss: 0.8698\n",
            "Epoch: [411/1000] Step: [20960] Loss: 0.9039 Val Loss: 0.8636\n",
            "Epoch: [412/1000] Step: [20980] Loss: 0.9963 Val Loss: 0.8628\n",
            "Epoch: [412/1000] Step: [21000] Loss: 0.9333 Val Loss: 0.8617\n",
            "Epoch: [413/1000] Step: [21020] Loss: 0.9643 Val Loss: 0.8627\n",
            "Epoch: [413/1000] Step: [21040] Loss: 0.9165 Val Loss: 0.8536\n",
            "Epoch: [413/1000] Step: [21060] Loss: 0.9276 Val Loss: 0.8558\n",
            "Epoch: [414/1000] Step: [21080] Loss: 0.9637 Val Loss: 0.8563\n",
            "Epoch: [414/1000] Step: [21100] Loss: 0.9359 Val Loss: 0.8547\n",
            "Epoch: [415/1000] Step: [21120] Loss: 0.8786 Val Loss: 0.8476\n",
            "Epoch: [415/1000] Step: [21140] Loss: 0.8931 Val Loss: 0.8485\n",
            "Epoch: [415/1000] Step: [21160] Loss: 0.9013 Val Loss: 0.8496\n",
            "Epoch: [416/1000] Step: [21180] Loss: 0.9322 Val Loss: 0.8637\n",
            "Epoch: [416/1000] Step: [21200] Loss: 0.9038 Val Loss: 0.8504\n",
            "Epoch: [417/1000] Step: [21220] Loss: 0.9112 Val Loss: 0.8609\n",
            "Epoch: [417/1000] Step: [21240] Loss: 0.9082 Val Loss: 0.8547\n",
            "Epoch: [417/1000] Step: [21260] Loss: 0.9257 Val Loss: 0.8619\n",
            "Epoch: [418/1000] Step: [21280] Loss: 0.9343 Val Loss: 0.8610\n",
            "Epoch: [418/1000] Step: [21300] Loss: 0.9263 Val Loss: 0.8488\n",
            "Epoch: [419/1000] Step: [21320] Loss: 0.9273 Val Loss: 0.8498\n",
            "Epoch: [419/1000] Step: [21340] Loss: 0.9891 Val Loss: 0.8574\n",
            "Epoch: [419/1000] Step: [21360] Loss: 0.9325 Val Loss: 0.8621\n",
            "Epoch: [420/1000] Step: [21380] Loss: 0.9911 Val Loss: 0.8927\n",
            "Epoch: [420/1000] Step: [21400] Loss: 0.9561 Val Loss: 0.8810\n",
            "Epoch: [420/1000] Step: [21420] Loss: 1.0854 Val Loss: 0.8679\n",
            "Epoch: [421/1000] Step: [21440] Loss: 0.9620 Val Loss: 0.8724\n",
            "Epoch: [421/1000] Step: [21460] Loss: 0.8872 Val Loss: 0.8672\n",
            "Epoch: [422/1000] Step: [21480] Loss: 0.9507 Val Loss: 0.8679\n",
            "Epoch: [422/1000] Step: [21500] Loss: 0.9334 Val Loss: 0.8649\n",
            "Epoch: [422/1000] Step: [21520] Loss: 0.9369 Val Loss: 0.8595\n",
            "Epoch: [423/1000] Step: [21540] Loss: 0.9479 Val Loss: 0.8624\n",
            "Epoch: [423/1000] Step: [21560] Loss: 0.9520 Val Loss: 0.8626\n",
            "Epoch: [424/1000] Step: [21580] Loss: 0.8979 Val Loss: 0.8627\n",
            "Epoch: [424/1000] Step: [21600] Loss: 0.9332 Val Loss: 0.8679\n",
            "Epoch: [424/1000] Step: [21620] Loss: 0.8852 Val Loss: 0.8559\n",
            "Epoch: [425/1000] Step: [21640] Loss: 0.9565 Val Loss: 0.8739\n",
            "Epoch: [425/1000] Step: [21660] Loss: 0.9422 Val Loss: 0.8655\n",
            "Epoch: [426/1000] Step: [21680] Loss: 0.9108 Val Loss: 0.8677\n",
            "Epoch: [426/1000] Step: [21700] Loss: 0.9319 Val Loss: 0.8590\n",
            "Epoch: [426/1000] Step: [21720] Loss: 0.9583 Val Loss: 0.8630\n",
            "Epoch: [427/1000] Step: [21740] Loss: 0.9462 Val Loss: 0.8545\n",
            "Epoch: [427/1000] Step: [21760] Loss: 0.8910 Val Loss: 0.8574\n",
            "Epoch: [428/1000] Step: [21780] Loss: 0.9189 Val Loss: 0.8550\n",
            "Epoch: [428/1000] Step: [21800] Loss: 0.9202 Val Loss: 0.8547\n",
            "Epoch: [428/1000] Step: [21820] Loss: 0.9530 Val Loss: 0.8575\n",
            "Epoch: [429/1000] Step: [21840] Loss: 0.9871 Val Loss: 0.8578\n",
            "Epoch: [429/1000] Step: [21860] Loss: 0.9293 Val Loss: 0.8574\n",
            "Epoch: [430/1000] Step: [21880] Loss: 0.9160 Val Loss: 0.8560\n",
            "Epoch: [430/1000] Step: [21900] Loss: 0.9671 Val Loss: 0.8562\n",
            "Epoch: [430/1000] Step: [21920] Loss: 0.9371 Val Loss: 0.8494\n",
            "Epoch: [431/1000] Step: [21940] Loss: 0.9354 Val Loss: 0.8549\n",
            "Epoch: [431/1000] Step: [21960] Loss: 0.9094 Val Loss: 0.8651\n",
            "Epoch: [431/1000] Step: [21980] Loss: 0.8734 Val Loss: 0.8528\n",
            "Epoch: [432/1000] Step: [22000] Loss: 0.9517 Val Loss: 0.8510\n",
            "Epoch: [432/1000] Step: [22020] Loss: 0.9286 Val Loss: 0.8606\n",
            "Epoch: [433/1000] Step: [22040] Loss: 0.9295 Val Loss: 0.8554\n",
            "Epoch: [433/1000] Step: [22060] Loss: 0.9115 Val Loss: 0.8477\n",
            "Epoch: [433/1000] Step: [22080] Loss: 0.9217 Val Loss: 0.8458\n",
            "Epoch: [434/1000] Step: [22100] Loss: 0.9250 Val Loss: 0.8510\n",
            "Epoch: [434/1000] Step: [22120] Loss: 0.9170 Val Loss: 0.8540\n",
            "Epoch: [435/1000] Step: [22140] Loss: 0.8447 Val Loss: 0.8472\n",
            "Epoch: [435/1000] Step: [22160] Loss: 0.8809 Val Loss: 0.8546\n",
            "Epoch: [435/1000] Step: [22180] Loss: 0.8980 Val Loss: 0.8522\n",
            "Epoch: [436/1000] Step: [22200] Loss: 0.9183 Val Loss: 0.8546\n",
            "Epoch: [436/1000] Step: [22220] Loss: 0.9140 Val Loss: 0.8489\n",
            "Epoch: [437/1000] Step: [22240] Loss: 0.8950 Val Loss: 0.8506\n",
            "Epoch: [437/1000] Step: [22260] Loss: 0.8991 Val Loss: 0.8488\n",
            "Epoch: [437/1000] Step: [22280] Loss: 0.8994 Val Loss: 0.8505\n",
            "Epoch: [438/1000] Step: [22300] Loss: 0.9280 Val Loss: 0.8464\n",
            "Epoch: [438/1000] Step: [22320] Loss: 0.8723 Val Loss: 0.8509\n",
            "Epoch: [439/1000] Step: [22340] Loss: 0.9068 Val Loss: 0.8485\n",
            "Epoch: [439/1000] Step: [22360] Loss: 0.9634 Val Loss: 0.8487\n",
            "Epoch: [439/1000] Step: [22380] Loss: 0.9198 Val Loss: 0.8497\n",
            "Epoch: [440/1000] Step: [22400] Loss: 0.9547 Val Loss: 0.8505\n",
            "Epoch: [440/1000] Step: [22420] Loss: 0.8978 Val Loss: 0.8452\n",
            "Epoch: [440/1000] Step: [22440] Loss: 1.0437 Val Loss: 0.8494\n",
            "Epoch: [441/1000] Step: [22460] Loss: 0.9134 Val Loss: 0.8492\n",
            "Epoch: [441/1000] Step: [22480] Loss: 0.8568 Val Loss: 0.8595\n",
            "Epoch: [442/1000] Step: [22500] Loss: 0.9258 Val Loss: 0.8605\n",
            "Epoch: [442/1000] Step: [22520] Loss: 0.9372 Val Loss: 0.8564\n",
            "Epoch: [442/1000] Step: [22540] Loss: 0.9223 Val Loss: 0.8506\n",
            "Epoch: [443/1000] Step: [22560] Loss: 0.9444 Val Loss: 0.8555\n",
            "Epoch: [443/1000] Step: [22580] Loss: 0.9297 Val Loss: 0.8548\n",
            "Epoch: [444/1000] Step: [22600] Loss: 0.8894 Val Loss: 0.8542\n",
            "Epoch: [444/1000] Step: [22620] Loss: 0.9242 Val Loss: 0.8587\n",
            "Epoch: [444/1000] Step: [22640] Loss: 0.8820 Val Loss: 0.8574\n",
            "Epoch: [445/1000] Step: [22660] Loss: 0.9456 Val Loss: 0.8568\n",
            "Epoch: [445/1000] Step: [22680] Loss: 0.9394 Val Loss: 0.8703\n",
            "Epoch: [446/1000] Step: [22700] Loss: 0.9004 Val Loss: 0.8725\n",
            "Epoch: [446/1000] Step: [22720] Loss: 0.9223 Val Loss: 0.8760\n",
            "Epoch: [446/1000] Step: [22740] Loss: 0.9648 Val Loss: 0.8711\n",
            "Epoch: [447/1000] Step: [22760] Loss: 0.9492 Val Loss: 0.8721\n",
            "Epoch: [447/1000] Step: [22780] Loss: 0.9259 Val Loss: 0.8800\n",
            "Epoch: [448/1000] Step: [22800] Loss: 0.9413 Val Loss: 0.8672\n",
            "Epoch: [448/1000] Step: [22820] Loss: 0.9545 Val Loss: 0.8644\n",
            "Epoch: [448/1000] Step: [22840] Loss: 0.9569 Val Loss: 0.8625\n",
            "Epoch: [449/1000] Step: [22860] Loss: 0.9798 Val Loss: 0.8656\n",
            "Epoch: [449/1000] Step: [22880] Loss: 0.9392 Val Loss: 0.8633\n",
            "Epoch: [450/1000] Step: [22900] Loss: 0.9193 Val Loss: 0.8620\n",
            "Epoch: [450/1000] Step: [22920] Loss: 0.9688 Val Loss: 0.8577\n",
            "Epoch: [450/1000] Step: [22940] Loss: 0.9341 Val Loss: 0.8554\n",
            "Epoch: [451/1000] Step: [22960] Loss: 0.9388 Val Loss: 0.8546\n",
            "Epoch: [451/1000] Step: [22980] Loss: 0.9191 Val Loss: 0.8570\n",
            "Epoch: [451/1000] Step: [23000] Loss: 0.8859 Val Loss: 0.8560\n",
            "Epoch: [452/1000] Step: [23020] Loss: 0.9591 Val Loss: 0.8549\n",
            "Epoch: [452/1000] Step: [23040] Loss: 0.9273 Val Loss: 0.8527\n",
            "Epoch: [453/1000] Step: [23060] Loss: 0.9492 Val Loss: 0.8527\n",
            "Epoch: [453/1000] Step: [23080] Loss: 0.9009 Val Loss: 0.8471\n",
            "Epoch: [453/1000] Step: [23100] Loss: 0.9168 Val Loss: 0.8518\n",
            "Epoch: [454/1000] Step: [23120] Loss: 0.9385 Val Loss: 0.8424\n",
            "Epoch: [454/1000] Step: [23140] Loss: 0.9187 Val Loss: 0.8449\n",
            "Epoch: [455/1000] Step: [23160] Loss: 0.8874 Val Loss: 0.8473\n",
            "Epoch: [455/1000] Step: [23180] Loss: 0.8819 Val Loss: 0.8466\n",
            "Epoch: [455/1000] Step: [23200] Loss: 0.8824 Val Loss: 0.8433\n",
            "Epoch: [456/1000] Step: [23220] Loss: 0.8957 Val Loss: 0.8417\n",
            "Epoch: [456/1000] Step: [23240] Loss: 0.9098 Val Loss: 0.8476\n",
            "Epoch: [457/1000] Step: [23260] Loss: 0.8984 Val Loss: 0.8514\n",
            "Epoch: [457/1000] Step: [23280] Loss: 0.9089 Val Loss: 0.8483\n",
            "Epoch: [457/1000] Step: [23300] Loss: 0.9349 Val Loss: 0.8469\n",
            "Epoch: [458/1000] Step: [23320] Loss: 0.9292 Val Loss: 0.8444\n",
            "Epoch: [458/1000] Step: [23340] Loss: 0.8997 Val Loss: 0.8484\n",
            "Epoch: [459/1000] Step: [23360] Loss: 0.8982 Val Loss: 0.8497\n",
            "Epoch: [459/1000] Step: [23380] Loss: 0.9565 Val Loss: 0.8495\n",
            "Epoch: [459/1000] Step: [23400] Loss: 0.9123 Val Loss: 0.8467\n",
            "Epoch: [460/1000] Step: [23420] Loss: 0.9319 Val Loss: 0.8461\n",
            "Epoch: [460/1000] Step: [23440] Loss: 0.9130 Val Loss: 0.8441\n",
            "Epoch: [460/1000] Step: [23460] Loss: 1.0391 Val Loss: 0.8421\n",
            "Epoch: [461/1000] Step: [23480] Loss: 0.9316 Val Loss: 0.8432\n",
            "Epoch: [461/1000] Step: [23500] Loss: 0.8693 Val Loss: 0.8405\n",
            "Epoch: [462/1000] Step: [23520] Loss: 0.9081 Val Loss: 0.8457\n",
            "Epoch: [462/1000] Step: [23540] Loss: 0.9155 Val Loss: 0.8513\n",
            "Epoch: [462/1000] Step: [23560] Loss: 0.9215 Val Loss: 0.8472\n",
            "Epoch: [463/1000] Step: [23580] Loss: 0.9353 Val Loss: 0.8515\n",
            "Epoch: [463/1000] Step: [23600] Loss: 0.9301 Val Loss: 0.8579\n",
            "Epoch: [464/1000] Step: [23620] Loss: 0.8785 Val Loss: 0.8485\n",
            "Epoch: [464/1000] Step: [23640] Loss: 0.9092 Val Loss: 0.8584\n",
            "Epoch: [464/1000] Step: [23660] Loss: 0.8933 Val Loss: 0.8720\n",
            "Epoch: [465/1000] Step: [23680] Loss: 0.9479 Val Loss: 0.8723\n",
            "Epoch: [465/1000] Step: [23700] Loss: 0.9221 Val Loss: 0.8571\n",
            "Epoch: [466/1000] Step: [23720] Loss: 0.8899 Val Loss: 0.8641\n",
            "Epoch: [466/1000] Step: [23740] Loss: 0.9208 Val Loss: 0.8582\n",
            "Epoch: [466/1000] Step: [23760] Loss: 0.9562 Val Loss: 0.8583\n",
            "Epoch: [467/1000] Step: [23780] Loss: 0.9196 Val Loss: 0.8508\n",
            "Epoch: [467/1000] Step: [23800] Loss: 0.9015 Val Loss: 0.8557\n",
            "Epoch: [468/1000] Step: [23820] Loss: 0.9202 Val Loss: 0.8571\n",
            "Epoch: [468/1000] Step: [23840] Loss: 0.9216 Val Loss: 0.8498\n",
            "Epoch: [468/1000] Step: [23860] Loss: 0.9461 Val Loss: 0.8483\n",
            "Epoch: [469/1000] Step: [23880] Loss: 0.9569 Val Loss: 0.8507\n",
            "Epoch: [469/1000] Step: [23900] Loss: 0.9194 Val Loss: 0.8501\n",
            "Epoch: [470/1000] Step: [23920] Loss: 0.8955 Val Loss: 0.8500\n",
            "Epoch: [470/1000] Step: [23940] Loss: 0.9606 Val Loss: 0.8514\n",
            "Epoch: [470/1000] Step: [23960] Loss: 0.9151 Val Loss: 0.8510\n",
            "Epoch: [471/1000] Step: [23980] Loss: 0.9293 Val Loss: 0.8552\n",
            "Epoch: [471/1000] Step: [24000] Loss: 0.9395 Val Loss: 0.8603\n",
            "Epoch: [471/1000] Step: [24020] Loss: 0.8661 Val Loss: 0.8523\n",
            "Epoch: [472/1000] Step: [24040] Loss: 0.9693 Val Loss: 0.8578\n",
            "Epoch: [472/1000] Step: [24060] Loss: 0.9410 Val Loss: 0.8503\n",
            "Epoch: [473/1000] Step: [24080] Loss: 0.9316 Val Loss: 0.8534\n",
            "Epoch: [473/1000] Step: [24100] Loss: 0.9171 Val Loss: 0.8585\n",
            "Epoch: [473/1000] Step: [24120] Loss: 0.9289 Val Loss: 0.8609\n",
            "Epoch: [474/1000] Step: [24140] Loss: 0.9655 Val Loss: 0.8583\n",
            "Epoch: [474/1000] Step: [24160] Loss: 0.9383 Val Loss: 0.8630\n",
            "Epoch: [475/1000] Step: [24180] Loss: 0.8762 Val Loss: 0.8550\n",
            "Epoch: [475/1000] Step: [24200] Loss: 0.9145 Val Loss: 0.8578\n",
            "Epoch: [475/1000] Step: [24220] Loss: 0.9294 Val Loss: 0.8649\n",
            "Epoch: [476/1000] Step: [24240] Loss: 0.9459 Val Loss: 0.8664\n",
            "Epoch: [476/1000] Step: [24260] Loss: 0.9597 Val Loss: 0.8988\n",
            "Epoch: [477/1000] Step: [24280] Loss: 0.9663 Val Loss: 0.8830\n",
            "Epoch: [477/1000] Step: [24300] Loss: 0.9372 Val Loss: 0.8713\n",
            "Epoch: [477/1000] Step: [24320] Loss: 0.9609 Val Loss: 0.8713\n",
            "Epoch: [478/1000] Step: [24340] Loss: 0.9651 Val Loss: 0.8669\n",
            "Epoch: [478/1000] Step: [24360] Loss: 0.9445 Val Loss: 0.8706\n",
            "Epoch: [479/1000] Step: [24380] Loss: 0.9305 Val Loss: 0.8613\n",
            "Epoch: [479/1000] Step: [24400] Loss: 0.9857 Val Loss: 0.8679\n",
            "Epoch: [479/1000] Step: [24420] Loss: 0.9521 Val Loss: 0.8638\n",
            "Epoch: [480/1000] Step: [24440] Loss: 0.9498 Val Loss: 0.8651\n",
            "Epoch: [480/1000] Step: [24460] Loss: 0.9310 Val Loss: 0.8663\n",
            "Epoch: [480/1000] Step: [24480] Loss: 1.0431 Val Loss: 0.8594\n",
            "Epoch: [481/1000] Step: [24500] Loss: 0.9609 Val Loss: 0.8580\n",
            "Epoch: [481/1000] Step: [24520] Loss: 0.8881 Val Loss: 0.8612\n",
            "Epoch: [482/1000] Step: [24540] Loss: 0.9546 Val Loss: 0.8596\n",
            "Epoch: [482/1000] Step: [24560] Loss: 0.9388 Val Loss: 0.8639\n",
            "Epoch: [482/1000] Step: [24580] Loss: 0.9263 Val Loss: 0.8571\n",
            "Epoch: [483/1000] Step: [24600] Loss: 0.9455 Val Loss: 0.8673\n",
            "Epoch: [483/1000] Step: [24620] Loss: 0.9325 Val Loss: 0.8590\n",
            "Epoch: [484/1000] Step: [24640] Loss: 0.8841 Val Loss: 0.8565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-330-303ce750b0b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#train(model, X, y, batch_size=128, n_epochs=EPOCHS10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-303-571f46fac3ce>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, batch_size, seq_length, nb_epochs, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWmLOdgGyGQH",
        "colab_type": "text"
      },
      "source": [
        "# 5. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEGkISbDyF0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': model.hidden_size,\n",
        "              'n_layers': model.num_layers,\n",
        "              'state_dict': model.state_dict(),\n",
        "              'tokens': model.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_sqtcvjyBOV",
        "colab_type": "text"
      },
      "source": [
        "# 5. Generate samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0BQGwWUylhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        # tensor inputs\n",
        "        x = np.array([[model.char2int[char]]])\n",
        "        x = one_hot_encode(x, model.chars)\n",
        "        inputs = torch.from_numpy(x).float()\n",
        "        \n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data.float() for each in h])\n",
        "        \n",
        "        # get the output of the model\n",
        "        out, h = model(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        \n",
        "        p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(model.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return model.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQPQ6mp9y3ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(model, size, sentence='The', top_k=None):\n",
        "    \n",
        "    model = model.to(device)\n",
        "    \n",
        "    model.eval() # eval mode\n",
        "    \n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    chars = [ch for ch in sentence]\n",
        "    \n",
        "    h = model.init_hidden(1)\n",
        "    \n",
        "    for ch in sentence:\n",
        "        char, h = predict(model, ch, h, top_k=top_k)\n",
        "    \n",
        "    #add last one\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for i in range(size):\n",
        "        char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpautWFbzHDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "4d580b90-87f9-48b1-e07e-fa5d8bc6f475"
      },
      "source": [
        "print(sample(model, 1000, prime='hey you', top_k=5))"
      ],
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hey you can't stard \n",
            " \n",
            " somewhere through the tame \n",
            " we see who you are \n",
            " it is my heart \n",
            " and if it is true \n",
            " and we are so cald, they wanna stay, i've come down with love, \n",
            " i've come down with love, \n",
            " i am gonna be \n",
            " into your wings on us \n",
            " you will call me when i am an idiot \n",
            " they are gonna give up tired tonight \n",
            " i am a long bollablit still \n",
            " i am a stuck on your wines \n",
            " i am taking up to me \n",
            " \n",
            " i can't she can we see why it is mederar tight, \n",
            " and the sang shake the surface to find the clace, though \n",
            " and all the stars all there we could be my home \n",
            " i wanna be a chill far \n",
            " i've been broken and singen away \n",
            " that won't get it and see \n",
            " and i will see you something that we are on \n",
            " i am a some cause i can't go again \n",
            " and you are always meant to be \n",
            " your stally saved to be if your time \n",
            " and if i ever cancked and see \n",
            " and i see it is a second open open, but take all a sound \n",
            " the time and all the world all your life \n",
            " the same \n",
            " \n",
            " if you are starting away \n",
            " and you could be mear \n",
            " wit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXlUnEvhE4NP",
        "colab_type": "code",
        "outputId": "f7e890aa-1dbb-4698-e12b-049d169a69fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "    print()\n",
        "    print('----- diversity:', diversity)\n",
        "\n",
        "    generated = ''\n",
        "    # insert your 40-chars long string. OBS it needs to be exactly 40 chars!\n",
        "    sentence = \"Hey my lovely dear, i missed you so much\"\n",
        "    sentence = sentence.lower() #stop here-------------^\"\n",
        "    generated += sentence\n",
        "\n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "\n",
        "    for i in range(400):\n",
        "        x = np.zeros((1, SEQUENCE_LENGTH, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[0, t, char_to_index[char]] = 1\n",
        "        x = torch.from_numpy(x)\n",
        "        x = x.float()\n",
        "        x = x.to(device)\n",
        "        model.eval()\n",
        "        #x_input = torch.tensor([70, 80, 90]).float().reshape((1, 3, 1))\n",
        "        #yhat = model(x_input)\n",
        "        #print(yhat)\n",
        "        predictions = model(x)[0]\n",
        "        predictions = predictions.cpu().detach()\n",
        "        predictions = predictions.numpy()\n",
        "        #next_index = sample(predictions, diversity)\n",
        "        next_index = np.argmax(predictions)\n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"hey my lovely dear, i missed you so much\"\n",
            "hey my lovely dear, i missed you so much"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-290-b7a04d23c7ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SEQUENCE_LENGTH' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmZd4-npZnvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds =preds.cpu()\n",
        "    preds = preds.detach()\n",
        "    print(preds.shape)\n",
        "    print(preds)\n",
        "    if temperature == 0:\n",
        "        temperature = 1\n",
        "\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}